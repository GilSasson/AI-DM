{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Keras_First_NN_HW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GilSasson/ai/blob/main/Keras_First_NN_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH89xMC-RcRv"
      },
      "source": [
        "## Using Keras to Build and Train Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsD6eEeMRcR0"
      },
      "source": [
        "This exercise is based on Intel's course at: https://software.intel.com/content/dam/develop/public/us/en/downloads/intel-dl101-class5.zip and at: https://software.intel.com/content/www/us/en/develop/training/course-deep-learning.html. \n",
        "\n",
        "We will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
        "\n",
        "## UCI Pima Diabetes Dataset\n",
        "\n",
        "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
        "\n",
        "\n",
        "### Attributes: (all numeric-valued)\n",
        "   1. Number of times pregnant\n",
        "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "   3. Diastolic blood pressure (mm Hg)\n",
        "   4. Triceps skin fold thickness (mm)\n",
        "   5. 2-Hour serum insulin (mu U/ml)\n",
        "   6. Body mass index (weight in kg/(height in m)^2)\n",
        "   7. Diabetes pedigree function\n",
        "   8. Age (years)\n",
        "   9. Class variable (0 or 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYTsV1DjRcR0"
      },
      "source": [
        "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vLmSDJg8RcR1"
      },
      "source": [
        "#Preliminaries\n",
        "\n",
        "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvLECRWDRcR1"
      },
      "source": [
        "## Import Keras objects for Deep Learning\n",
        "\n",
        "from keras.models  import Sequential\n",
        "#from keras import backend as K\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam, SGD, RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mIzX1Em8RcR1"
      },
      "source": [
        "## Load in the data set (Internet Access needed)\n",
        "url = 'https://github.com/rosenfa/nn/blob/master/pima-indians-diabetes.csv?raw=true'\n",
        "diabetes_df=pd.read_csv(url,  header=0, error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "YFm4IprGRcR2",
        "outputId": "63fed322-3f9b-4757-9c76-caefd208924e"
      },
      "source": [
        "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
        "print(diabetes_df.shape)\n",
        "diabetes_df.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>9</td>\n",
              "      <td>145</td>\n",
              "      <td>80</td>\n",
              "      <td>46</td>\n",
              "      <td>130</td>\n",
              "      <td>37.9</td>\n",
              "      <td>0.637</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>614</th>\n",
              "      <td>11</td>\n",
              "      <td>138</td>\n",
              "      <td>74</td>\n",
              "      <td>26</td>\n",
              "      <td>144</td>\n",
              "      <td>36.1</td>\n",
              "      <td>0.557</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>2</td>\n",
              "      <td>141</td>\n",
              "      <td>58</td>\n",
              "      <td>34</td>\n",
              "      <td>128</td>\n",
              "      <td>25.4</td>\n",
              "      <td>0.699</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>3</td>\n",
              "      <td>158</td>\n",
              "      <td>76</td>\n",
              "      <td>36</td>\n",
              "      <td>245</td>\n",
              "      <td>31.6</td>\n",
              "      <td>0.851</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>68</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>24.5</td>\n",
              "      <td>1.138</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  ...  Age  Outcome\n",
              "663            9      145  ...   40        1\n",
              "614           11      138  ...   50        1\n",
              "63             2      141  ...   24        0\n",
              "31             3      158  ...   28        1\n",
              "434            1       90  ...   36        0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "QsbBa-otRcR2"
      },
      "source": [
        "X = np.asarray(diabetes_df.drop('Outcome',1))\n",
        "y = np.asarray(diabetes_df['Outcome'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "s3652HjFRcR2"
      },
      "source": [
        "# Split the data to Train, and Test (75%, 25%)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdYKCFx4RcR2",
        "outputId": "ecaa7234-5b7c-48e0-f4ff-6a7cb0a0a8b2"
      },
      "source": [
        "np.mean(y), np.mean(1-y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3489583333333333, 0.6510416666666666)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFVKLmW4RcR2"
      },
      "source": [
        "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
        "## Exercise: Get a baseline performance using Random Forest\n",
        "To begin, and get a baseline for classifier performance:\n",
        "1. Train a Random Forest model with 200 trees on the training data.\n",
        "2. Calculate the accuracy and roc_auc_score of the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1th7VHwiRcR3",
        "outputId": "2445fd91-4e46-40b0-8269-fbcb4537498e"
      },
      "source": [
        "## Train the RF Model\n",
        "rf_model = RandomForestClassifier(n_estimators=200)\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gRS_HmlRcR3",
        "outputId": "e384f2cb-d644-4e76-c179-6b4b77a01ce9"
      },
      "source": [
        "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
        "y_pred_class_rf = rf_model.predict(X_test)\n",
        "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
        "\n",
        "\n",
        "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
        "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 0.776\n",
            "roc-auc is 0.825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "-sV-PzOaRcR3",
        "outputId": "90e7cecb-08bd-4ef6-c502-d308abdf689b"
      },
      "source": [
        "def plot_roc(y_test, y_pred, model_name):\n",
        "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.plot(fpr, tpr, 'k-')\n",
        "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
        "    ax.grid(True)\n",
        "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
        "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
        "\n",
        "\n",
        "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4wvhC7yRcR4"
      },
      "source": [
        "## Build a Single Hidden Layer Neural Network\n",
        "\n",
        "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "j9FHH1ZdRcR4"
      },
      "source": [
        "## First let's normalize the data\n",
        "## This aids the training of neural nets by providing numerical stability\n",
        "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
        "\n",
        "\n",
        "normalizer = StandardScaler()\n",
        "X_train_norm = normalizer.fit_transform(X_train)\n",
        "X_test_norm = normalizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CutQ8tpPMWWO"
      },
      "source": [
        "Keras has many shapes.  See: https://keras.io/api/layers/core_layers/dense/ for information about \"dense\" and sequential means a \"basic\" ordering (as opposed to more complicated ones we'll learn about starting next week)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "voirYVtORcR4"
      },
      "source": [
        "# Define the Model \n",
        "# Input size is 8-dimensional\n",
        "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
        "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
        "\n",
        "model_1 = Sequential([\n",
        "    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q08m_L6RcR4",
        "outputId": "4691cf77-686f-44d6-ae3b-204540647005"
      },
      "source": [
        "#  This is a nice tool to view the model you have created and count the parameters\n",
        "\n",
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 12)                108       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 121\n",
            "Trainable params: 121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HwBPvVvRcR4"
      },
      "source": [
        "### Comprehension question:\n",
        "Why do we have 121 parameters?  Does that make sense?\n",
        "Think about why there are 108 parameters in first layer when only 8 features exist and the network has 12 nodes (8*12 is NOT 108), and why the output layer has 13 values for only 12 nodes.\n",
        "\n",
        "\n",
        "Let's fit our model for 200 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-o8dQzhRcR5",
        "outputId": "f74a925d-160c-4015-8a94-7eab66059929"
      },
      "source": [
        "# Fit(Train) the Model\n",
        "\n",
        "# Compile the model with Optimizer, Loss Function and Metrics\n",
        "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
        "\n",
        "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
        "# the fit function returns the run history. \n",
        "# It is very convenient, as it contains information about the model fit, iterations etc."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.7599 - accuracy: 0.5136 - val_loss: 0.7788 - val_accuracy: 0.5104\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7247 - accuracy: 0.5862 - val_loss: 0.7704 - val_accuracy: 0.5104\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7426 - accuracy: 0.5638 - val_loss: 0.7623 - val_accuracy: 0.5208\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7520 - accuracy: 0.5640 - val_loss: 0.7546 - val_accuracy: 0.5312\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7065 - accuracy: 0.5964 - val_loss: 0.7472 - val_accuracy: 0.5260\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7310 - accuracy: 0.5537 - val_loss: 0.7401 - val_accuracy: 0.5312\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7097 - accuracy: 0.5856 - val_loss: 0.7333 - val_accuracy: 0.5365\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7049 - accuracy: 0.6026 - val_loss: 0.7267 - val_accuracy: 0.5469\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7096 - accuracy: 0.5884 - val_loss: 0.7205 - val_accuracy: 0.5521\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6705 - accuracy: 0.6312 - val_loss: 0.7144 - val_accuracy: 0.5625\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6795 - accuracy: 0.5981 - val_loss: 0.7086 - val_accuracy: 0.5677\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6750 - accuracy: 0.6052 - val_loss: 0.7031 - val_accuracy: 0.5729\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6787 - accuracy: 0.6165 - val_loss: 0.6976 - val_accuracy: 0.5729\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6846 - accuracy: 0.5832 - val_loss: 0.6924 - val_accuracy: 0.5729\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6748 - accuracy: 0.6092 - val_loss: 0.6874 - val_accuracy: 0.5729\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6708 - accuracy: 0.6059 - val_loss: 0.6826 - val_accuracy: 0.5729\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6741 - accuracy: 0.5936 - val_loss: 0.6779 - val_accuracy: 0.5729\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6529 - accuracy: 0.6250 - val_loss: 0.6734 - val_accuracy: 0.5833\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6505 - accuracy: 0.6439 - val_loss: 0.6691 - val_accuracy: 0.5885\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6367 - accuracy: 0.6527 - val_loss: 0.6650 - val_accuracy: 0.5990\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6364 - accuracy: 0.6338 - val_loss: 0.6609 - val_accuracy: 0.5990\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6464 - accuracy: 0.6331 - val_loss: 0.6570 - val_accuracy: 0.5990\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6260 - accuracy: 0.6580 - val_loss: 0.6533 - val_accuracy: 0.5990\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6232 - accuracy: 0.6297 - val_loss: 0.6496 - val_accuracy: 0.5990\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6285 - accuracy: 0.6505 - val_loss: 0.6461 - val_accuracy: 0.6042\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6506 - accuracy: 0.6157 - val_loss: 0.6427 - val_accuracy: 0.6042\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6149 - accuracy: 0.6457 - val_loss: 0.6394 - val_accuracy: 0.6094\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6130 - accuracy: 0.6583 - val_loss: 0.6363 - val_accuracy: 0.6042\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6176 - accuracy: 0.6581 - val_loss: 0.6332 - val_accuracy: 0.6042\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5919 - accuracy: 0.6956 - val_loss: 0.6302 - val_accuracy: 0.6094\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5951 - accuracy: 0.6785 - val_loss: 0.6273 - val_accuracy: 0.6094\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6048 - accuracy: 0.6638 - val_loss: 0.6245 - val_accuracy: 0.6042\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6017 - accuracy: 0.6595 - val_loss: 0.6219 - val_accuracy: 0.6042\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6264 - accuracy: 0.6324 - val_loss: 0.6193 - val_accuracy: 0.6146\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6035 - accuracy: 0.6585 - val_loss: 0.6167 - val_accuracy: 0.6146\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6116 - accuracy: 0.6515 - val_loss: 0.6142 - val_accuracy: 0.6094\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5899 - accuracy: 0.6737 - val_loss: 0.6118 - val_accuracy: 0.6094\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5884 - accuracy: 0.6672 - val_loss: 0.6095 - val_accuracy: 0.6198\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5875 - accuracy: 0.6682 - val_loss: 0.6073 - val_accuracy: 0.6250\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5968 - accuracy: 0.6599 - val_loss: 0.6051 - val_accuracy: 0.6354\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5946 - accuracy: 0.6651 - val_loss: 0.6029 - val_accuracy: 0.6354\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5692 - accuracy: 0.6952 - val_loss: 0.6008 - val_accuracy: 0.6406\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5879 - accuracy: 0.6512 - val_loss: 0.5988 - val_accuracy: 0.6406\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5967 - accuracy: 0.6313 - val_loss: 0.5968 - val_accuracy: 0.6458\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5986 - accuracy: 0.6468 - val_loss: 0.5949 - val_accuracy: 0.6458\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5565 - accuracy: 0.6814 - val_loss: 0.5930 - val_accuracy: 0.6406\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5759 - accuracy: 0.6673 - val_loss: 0.5912 - val_accuracy: 0.6458\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5883 - accuracy: 0.6616 - val_loss: 0.5895 - val_accuracy: 0.6458\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5683 - accuracy: 0.6711 - val_loss: 0.5877 - val_accuracy: 0.6406\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5564 - accuracy: 0.6941 - val_loss: 0.5860 - val_accuracy: 0.6406\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5824 - accuracy: 0.6533 - val_loss: 0.5844 - val_accuracy: 0.6406\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5743 - accuracy: 0.6845 - val_loss: 0.5828 - val_accuracy: 0.6406\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5722 - accuracy: 0.6852 - val_loss: 0.5812 - val_accuracy: 0.6458\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5426 - accuracy: 0.7221 - val_loss: 0.5797 - val_accuracy: 0.6458\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5552 - accuracy: 0.6681 - val_loss: 0.5782 - val_accuracy: 0.6458\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5531 - accuracy: 0.6874 - val_loss: 0.5767 - val_accuracy: 0.6510\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5435 - accuracy: 0.7137 - val_loss: 0.5753 - val_accuracy: 0.6510\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5490 - accuracy: 0.6907 - val_loss: 0.5739 - val_accuracy: 0.6562\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5580 - accuracy: 0.6718 - val_loss: 0.5725 - val_accuracy: 0.6510\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5493 - accuracy: 0.7070 - val_loss: 0.5712 - val_accuracy: 0.6562\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5451 - accuracy: 0.6909 - val_loss: 0.5699 - val_accuracy: 0.6562\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5447 - accuracy: 0.6850 - val_loss: 0.5686 - val_accuracy: 0.6615\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5561 - accuracy: 0.6877 - val_loss: 0.5673 - val_accuracy: 0.6667\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5422 - accuracy: 0.6895 - val_loss: 0.5661 - val_accuracy: 0.6615\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5463 - accuracy: 0.6839 - val_loss: 0.5649 - val_accuracy: 0.6719\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5526 - accuracy: 0.6725 - val_loss: 0.5637 - val_accuracy: 0.6719\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5481 - accuracy: 0.6806 - val_loss: 0.5626 - val_accuracy: 0.6719\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5420 - accuracy: 0.6561 - val_loss: 0.5614 - val_accuracy: 0.6719\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5494 - accuracy: 0.6737 - val_loss: 0.5604 - val_accuracy: 0.6667\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5309 - accuracy: 0.7204 - val_loss: 0.5593 - val_accuracy: 0.6615\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5299 - accuracy: 0.7066 - val_loss: 0.5582 - val_accuracy: 0.6615\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5626 - accuracy: 0.6631 - val_loss: 0.5572 - val_accuracy: 0.6615\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5596 - accuracy: 0.6931 - val_loss: 0.5562 - val_accuracy: 0.6615\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5471 - accuracy: 0.6759 - val_loss: 0.5552 - val_accuracy: 0.6562\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5148 - accuracy: 0.7370 - val_loss: 0.5542 - val_accuracy: 0.6615\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5448 - accuracy: 0.6833 - val_loss: 0.5533 - val_accuracy: 0.6615\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5301 - accuracy: 0.6977 - val_loss: 0.5523 - val_accuracy: 0.6615\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5288 - accuracy: 0.7155 - val_loss: 0.5514 - val_accuracy: 0.6667\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5193 - accuracy: 0.7134 - val_loss: 0.5506 - val_accuracy: 0.6667\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5186 - accuracy: 0.7362 - val_loss: 0.5497 - val_accuracy: 0.6823\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5347 - accuracy: 0.7033 - val_loss: 0.5488 - val_accuracy: 0.6823\n",
            "Epoch 82/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5155 - accuracy: 0.7236 - val_loss: 0.5480 - val_accuracy: 0.6823\n",
            "Epoch 83/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5179 - accuracy: 0.7051 - val_loss: 0.5472 - val_accuracy: 0.6875\n",
            "Epoch 84/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5229 - accuracy: 0.7133 - val_loss: 0.5464 - val_accuracy: 0.6875\n",
            "Epoch 85/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5255 - accuracy: 0.6948 - val_loss: 0.5456 - val_accuracy: 0.6875\n",
            "Epoch 86/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5191 - accuracy: 0.7098 - val_loss: 0.5448 - val_accuracy: 0.6875\n",
            "Epoch 87/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5315 - accuracy: 0.6992 - val_loss: 0.5440 - val_accuracy: 0.6927\n",
            "Epoch 88/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5083 - accuracy: 0.7168 - val_loss: 0.5433 - val_accuracy: 0.6927\n",
            "Epoch 89/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5239 - accuracy: 0.6885 - val_loss: 0.5426 - val_accuracy: 0.6927\n",
            "Epoch 90/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5243 - accuracy: 0.6944 - val_loss: 0.5419 - val_accuracy: 0.6927\n",
            "Epoch 91/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5241 - accuracy: 0.6973 - val_loss: 0.5412 - val_accuracy: 0.6927\n",
            "Epoch 92/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5164 - accuracy: 0.7030 - val_loss: 0.5405 - val_accuracy: 0.6927\n",
            "Epoch 93/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5153 - accuracy: 0.7087 - val_loss: 0.5398 - val_accuracy: 0.6979\n",
            "Epoch 94/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5176 - accuracy: 0.7002 - val_loss: 0.5391 - val_accuracy: 0.6979\n",
            "Epoch 95/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5122 - accuracy: 0.7172 - val_loss: 0.5384 - val_accuracy: 0.6979\n",
            "Epoch 96/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5106 - accuracy: 0.7072 - val_loss: 0.5378 - val_accuracy: 0.6979\n",
            "Epoch 97/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5116 - accuracy: 0.7047 - val_loss: 0.5372 - val_accuracy: 0.6979\n",
            "Epoch 98/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5371 - accuracy: 0.6947 - val_loss: 0.5365 - val_accuracy: 0.6927\n",
            "Epoch 99/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5137 - accuracy: 0.7149 - val_loss: 0.5359 - val_accuracy: 0.6927\n",
            "Epoch 100/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5215 - accuracy: 0.7070 - val_loss: 0.5353 - val_accuracy: 0.6927\n",
            "Epoch 101/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5086 - accuracy: 0.7505 - val_loss: 0.5347 - val_accuracy: 0.6927\n",
            "Epoch 102/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5111 - accuracy: 0.7344 - val_loss: 0.5341 - val_accuracy: 0.6927\n",
            "Epoch 103/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5173 - accuracy: 0.7033 - val_loss: 0.5335 - val_accuracy: 0.6927\n",
            "Epoch 104/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5030 - accuracy: 0.7382 - val_loss: 0.5330 - val_accuracy: 0.6927\n",
            "Epoch 105/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5215 - accuracy: 0.7218 - val_loss: 0.5324 - val_accuracy: 0.6927\n",
            "Epoch 106/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5230 - accuracy: 0.7069 - val_loss: 0.5319 - val_accuracy: 0.6927\n",
            "Epoch 107/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5233 - accuracy: 0.7167 - val_loss: 0.5313 - val_accuracy: 0.6927\n",
            "Epoch 108/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5013 - accuracy: 0.7478 - val_loss: 0.5308 - val_accuracy: 0.6927\n",
            "Epoch 109/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5121 - accuracy: 0.7239 - val_loss: 0.5303 - val_accuracy: 0.6927\n",
            "Epoch 110/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5228 - accuracy: 0.7149 - val_loss: 0.5298 - val_accuracy: 0.6927\n",
            "Epoch 111/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5182 - accuracy: 0.7227 - val_loss: 0.5293 - val_accuracy: 0.6927\n",
            "Epoch 112/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5202 - accuracy: 0.7239 - val_loss: 0.5288 - val_accuracy: 0.6927\n",
            "Epoch 113/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5121 - accuracy: 0.7413 - val_loss: 0.5284 - val_accuracy: 0.6979\n",
            "Epoch 114/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5126 - accuracy: 0.7210 - val_loss: 0.5279 - val_accuracy: 0.6979\n",
            "Epoch 115/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5070 - accuracy: 0.7336 - val_loss: 0.5275 - val_accuracy: 0.7031\n",
            "Epoch 116/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4758 - accuracy: 0.7601 - val_loss: 0.5270 - val_accuracy: 0.7031\n",
            "Epoch 117/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4926 - accuracy: 0.7466 - val_loss: 0.5266 - val_accuracy: 0.7031\n",
            "Epoch 118/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5208 - accuracy: 0.7143 - val_loss: 0.5261 - val_accuracy: 0.7031\n",
            "Epoch 119/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5205 - accuracy: 0.7150 - val_loss: 0.5257 - val_accuracy: 0.7031\n",
            "Epoch 120/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5006 - accuracy: 0.7419 - val_loss: 0.5253 - val_accuracy: 0.6979\n",
            "Epoch 121/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5025 - accuracy: 0.7260 - val_loss: 0.5249 - val_accuracy: 0.6979\n",
            "Epoch 122/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4959 - accuracy: 0.7381 - val_loss: 0.5245 - val_accuracy: 0.6979\n",
            "Epoch 123/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4965 - accuracy: 0.7364 - val_loss: 0.5241 - val_accuracy: 0.6979\n",
            "Epoch 124/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5083 - accuracy: 0.7109 - val_loss: 0.5237 - val_accuracy: 0.7031\n",
            "Epoch 125/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5109 - accuracy: 0.7188 - val_loss: 0.5233 - val_accuracy: 0.7031\n",
            "Epoch 126/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5161 - accuracy: 0.7170 - val_loss: 0.5229 - val_accuracy: 0.7031\n",
            "Epoch 127/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5227 - accuracy: 0.7184 - val_loss: 0.5225 - val_accuracy: 0.7031\n",
            "Epoch 128/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4962 - accuracy: 0.7363 - val_loss: 0.5221 - val_accuracy: 0.7031\n",
            "Epoch 129/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5060 - accuracy: 0.7240 - val_loss: 0.5218 - val_accuracy: 0.7083\n",
            "Epoch 130/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4991 - accuracy: 0.7287 - val_loss: 0.5214 - val_accuracy: 0.7083\n",
            "Epoch 131/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5139 - accuracy: 0.7222 - val_loss: 0.5211 - val_accuracy: 0.7083\n",
            "Epoch 132/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4951 - accuracy: 0.7276 - val_loss: 0.5208 - val_accuracy: 0.7135\n",
            "Epoch 133/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4920 - accuracy: 0.7507 - val_loss: 0.5204 - val_accuracy: 0.7083\n",
            "Epoch 134/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4797 - accuracy: 0.7507 - val_loss: 0.5201 - val_accuracy: 0.7083\n",
            "Epoch 135/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4938 - accuracy: 0.7205 - val_loss: 0.5198 - val_accuracy: 0.7083\n",
            "Epoch 136/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4818 - accuracy: 0.7597 - val_loss: 0.5195 - val_accuracy: 0.7083\n",
            "Epoch 137/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5129 - accuracy: 0.7118 - val_loss: 0.5192 - val_accuracy: 0.7083\n",
            "Epoch 138/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4895 - accuracy: 0.7576 - val_loss: 0.5189 - val_accuracy: 0.7083\n",
            "Epoch 139/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4652 - accuracy: 0.7732 - val_loss: 0.5186 - val_accuracy: 0.7083\n",
            "Epoch 140/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4879 - accuracy: 0.7408 - val_loss: 0.5183 - val_accuracy: 0.7083\n",
            "Epoch 141/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5078 - accuracy: 0.7192 - val_loss: 0.5180 - val_accuracy: 0.7083\n",
            "Epoch 142/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4779 - accuracy: 0.7535 - val_loss: 0.5177 - val_accuracy: 0.7135\n",
            "Epoch 143/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4810 - accuracy: 0.7659 - val_loss: 0.5174 - val_accuracy: 0.7135\n",
            "Epoch 144/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4988 - accuracy: 0.7230 - val_loss: 0.5171 - val_accuracy: 0.7135\n",
            "Epoch 145/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4984 - accuracy: 0.7386 - val_loss: 0.5168 - val_accuracy: 0.7135\n",
            "Epoch 146/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4989 - accuracy: 0.7568 - val_loss: 0.5165 - val_accuracy: 0.7135\n",
            "Epoch 147/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4787 - accuracy: 0.7484 - val_loss: 0.5163 - val_accuracy: 0.7135\n",
            "Epoch 148/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4916 - accuracy: 0.7389 - val_loss: 0.5160 - val_accuracy: 0.7135\n",
            "Epoch 149/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5019 - accuracy: 0.7261 - val_loss: 0.5157 - val_accuracy: 0.7135\n",
            "Epoch 150/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5032 - accuracy: 0.7459 - val_loss: 0.5155 - val_accuracy: 0.7135\n",
            "Epoch 151/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5037 - accuracy: 0.7278 - val_loss: 0.5152 - val_accuracy: 0.7083\n",
            "Epoch 152/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5201 - accuracy: 0.7343 - val_loss: 0.5149 - val_accuracy: 0.7083\n",
            "Epoch 153/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.7555 - val_loss: 0.5147 - val_accuracy: 0.7083\n",
            "Epoch 154/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4882 - accuracy: 0.7620 - val_loss: 0.5144 - val_accuracy: 0.7083\n",
            "Epoch 155/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4996 - accuracy: 0.7213 - val_loss: 0.5142 - val_accuracy: 0.7083\n",
            "Epoch 156/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.7464 - val_loss: 0.5139 - val_accuracy: 0.7083\n",
            "Epoch 157/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5098 - accuracy: 0.7307 - val_loss: 0.5137 - val_accuracy: 0.7135\n",
            "Epoch 158/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4642 - accuracy: 0.7505 - val_loss: 0.5135 - val_accuracy: 0.7135\n",
            "Epoch 159/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5103 - accuracy: 0.7154 - val_loss: 0.5132 - val_accuracy: 0.7135\n",
            "Epoch 160/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4642 - accuracy: 0.7739 - val_loss: 0.5130 - val_accuracy: 0.7135\n",
            "Epoch 161/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5062 - accuracy: 0.7380 - val_loss: 0.5128 - val_accuracy: 0.7135\n",
            "Epoch 162/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4832 - accuracy: 0.7385 - val_loss: 0.5125 - val_accuracy: 0.7135\n",
            "Epoch 163/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4685 - accuracy: 0.7557 - val_loss: 0.5123 - val_accuracy: 0.7135\n",
            "Epoch 164/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5350 - accuracy: 0.7051 - val_loss: 0.5121 - val_accuracy: 0.7135\n",
            "Epoch 165/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4984 - accuracy: 0.7470 - val_loss: 0.5118 - val_accuracy: 0.7135\n",
            "Epoch 166/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5130 - accuracy: 0.7133 - val_loss: 0.5116 - val_accuracy: 0.7135\n",
            "Epoch 167/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5002 - accuracy: 0.7398 - val_loss: 0.5114 - val_accuracy: 0.7135\n",
            "Epoch 168/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4675 - accuracy: 0.7571 - val_loss: 0.5112 - val_accuracy: 0.7135\n",
            "Epoch 169/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4869 - accuracy: 0.7440 - val_loss: 0.5110 - val_accuracy: 0.7135\n",
            "Epoch 170/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4874 - accuracy: 0.7491 - val_loss: 0.5108 - val_accuracy: 0.7135\n",
            "Epoch 171/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4780 - accuracy: 0.7569 - val_loss: 0.5106 - val_accuracy: 0.7135\n",
            "Epoch 172/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4965 - accuracy: 0.7336 - val_loss: 0.5104 - val_accuracy: 0.7135\n",
            "Epoch 173/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4842 - accuracy: 0.7441 - val_loss: 0.5101 - val_accuracy: 0.7188\n",
            "Epoch 174/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4737 - accuracy: 0.7530 - val_loss: 0.5099 - val_accuracy: 0.7240\n",
            "Epoch 175/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4752 - accuracy: 0.7541 - val_loss: 0.5097 - val_accuracy: 0.7240\n",
            "Epoch 176/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4807 - accuracy: 0.7458 - val_loss: 0.5095 - val_accuracy: 0.7240\n",
            "Epoch 177/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.7085 - val_loss: 0.5093 - val_accuracy: 0.7240\n",
            "Epoch 178/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4890 - accuracy: 0.7442 - val_loss: 0.5091 - val_accuracy: 0.7240\n",
            "Epoch 179/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4720 - accuracy: 0.7618 - val_loss: 0.5090 - val_accuracy: 0.7240\n",
            "Epoch 180/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4821 - accuracy: 0.7395 - val_loss: 0.5088 - val_accuracy: 0.7292\n",
            "Epoch 181/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4740 - accuracy: 0.7600 - val_loss: 0.5086 - val_accuracy: 0.7292\n",
            "Epoch 182/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4919 - accuracy: 0.7579 - val_loss: 0.5084 - val_accuracy: 0.7292\n",
            "Epoch 183/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4772 - accuracy: 0.7546 - val_loss: 0.5082 - val_accuracy: 0.7344\n",
            "Epoch 184/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.7540 - val_loss: 0.5080 - val_accuracy: 0.7344\n",
            "Epoch 185/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4883 - accuracy: 0.7623 - val_loss: 0.5078 - val_accuracy: 0.7344\n",
            "Epoch 186/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4662 - accuracy: 0.7586 - val_loss: 0.5076 - val_accuracy: 0.7344\n",
            "Epoch 187/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4769 - accuracy: 0.7533 - val_loss: 0.5075 - val_accuracy: 0.7344\n",
            "Epoch 188/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4677 - accuracy: 0.7708 - val_loss: 0.5073 - val_accuracy: 0.7344\n",
            "Epoch 189/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4694 - accuracy: 0.7707 - val_loss: 0.5071 - val_accuracy: 0.7344\n",
            "Epoch 190/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4451 - accuracy: 0.7946 - val_loss: 0.5069 - val_accuracy: 0.7344\n",
            "Epoch 191/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4812 - accuracy: 0.7529 - val_loss: 0.5067 - val_accuracy: 0.7344\n",
            "Epoch 192/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5075 - accuracy: 0.7426 - val_loss: 0.5066 - val_accuracy: 0.7344\n",
            "Epoch 193/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4867 - accuracy: 0.7565 - val_loss: 0.5064 - val_accuracy: 0.7344\n",
            "Epoch 194/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4620 - accuracy: 0.7779 - val_loss: 0.5062 - val_accuracy: 0.7344\n",
            "Epoch 195/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5055 - accuracy: 0.7313 - val_loss: 0.5061 - val_accuracy: 0.7396\n",
            "Epoch 196/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4776 - accuracy: 0.7674 - val_loss: 0.5059 - val_accuracy: 0.7396\n",
            "Epoch 197/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4806 - accuracy: 0.7679 - val_loss: 0.5058 - val_accuracy: 0.7396\n",
            "Epoch 198/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5043 - accuracy: 0.7481 - val_loss: 0.5056 - val_accuracy: 0.7396\n",
            "Epoch 199/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4921 - accuracy: 0.7676 - val_loss: 0.5055 - val_accuracy: 0.7396\n",
            "Epoch 200/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4775 - accuracy: 0.7685 - val_loss: 0.5053 - val_accuracy: 0.7396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YY0HlLeRcR5"
      },
      "source": [
        "## Like we did for the Random Forest, we generate two kinds of predictions\n",
        "#  One is a hard decision, the other is a probabilitistic score.\n",
        "\n",
        "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
        "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRrd1m9JRcR5",
        "outputId": "5028611e-1343-464e-810d-5fc369b6900b"
      },
      "source": [
        "# Let's check out the outputs to get a feel for how keras apis work.\n",
        "y_pred_class_nn_1[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMJ7FZ-IRcR6",
        "outputId": "53eb612d-896d-4c71-9e12-c20e9de2eadc"
      },
      "source": [
        "y_pred_prob_nn_1[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.55834913],\n",
              "       [0.629658  ],\n",
              "       [0.3921829 ],\n",
              "       [0.4614403 ],\n",
              "       [0.17362997],\n",
              "       [0.46205387],\n",
              "       [0.03632483],\n",
              "       [0.4886    ],\n",
              "       [0.72788894],\n",
              "       [0.33776894]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "eqYsXBmPRcR6",
        "outputId": "9541f69c-fe31-427c-8035-41eaf49f8bb2"
      },
      "source": [
        "# Print model performance and plot the roc curve\n",
        "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
        "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
        "\n",
        "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 0.740\n",
            "roc-auc is 0.808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zNgwODgRcR6"
      },
      "source": [
        "There may be some variation in exact numbers due to randomness, but you should get results similar to the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koMTvJCwRcR6"
      },
      "source": [
        "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTGTNjsaRcR6",
        "outputId": "4daf8758-9aff-485b-90ad-36d0e80bd1a9"
      },
      "source": [
        "run_hist_1.history.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WgBL0UpRcR6"
      },
      "source": [
        "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "p2QOJA5nRcR6",
        "outputId": "1287ac46-7efc-4e9d-b979-f5678b9be9eb"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
        "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fcb01a63450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iq25zvARcR7"
      },
      "source": [
        "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oihHiNbMRcR7",
        "outputId": "6c96744e-ff59-4cd5-a85a-861aab9c3043"
      },
      "source": [
        "## Note that when we call \"fit\" again, it picks up where it left off\n",
        "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4794 - accuracy: 0.7639 - val_loss: 0.5052 - val_accuracy: 0.7396\n",
            "Epoch 2/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4792 - accuracy: 0.7639 - val_loss: 0.5050 - val_accuracy: 0.7396\n",
            "Epoch 3/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4790 - accuracy: 0.7639 - val_loss: 0.5049 - val_accuracy: 0.7396\n",
            "Epoch 4/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4787 - accuracy: 0.7639 - val_loss: 0.5047 - val_accuracy: 0.7396\n",
            "Epoch 5/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.7639 - val_loss: 0.5046 - val_accuracy: 0.7396\n",
            "Epoch 6/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4783 - accuracy: 0.7656 - val_loss: 0.5044 - val_accuracy: 0.7396\n",
            "Epoch 7/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4781 - accuracy: 0.7639 - val_loss: 0.5043 - val_accuracy: 0.7396\n",
            "Epoch 8/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4778 - accuracy: 0.7639 - val_loss: 0.5042 - val_accuracy: 0.7396\n",
            "Epoch 9/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4776 - accuracy: 0.7656 - val_loss: 0.5040 - val_accuracy: 0.7396\n",
            "Epoch 10/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4774 - accuracy: 0.7656 - val_loss: 0.5039 - val_accuracy: 0.7396\n",
            "Epoch 11/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4772 - accuracy: 0.7674 - val_loss: 0.5038 - val_accuracy: 0.7396\n",
            "Epoch 12/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4770 - accuracy: 0.7674 - val_loss: 0.5036 - val_accuracy: 0.7396\n",
            "Epoch 13/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4767 - accuracy: 0.7674 - val_loss: 0.5035 - val_accuracy: 0.7396\n",
            "Epoch 14/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4766 - accuracy: 0.7674 - val_loss: 0.5034 - val_accuracy: 0.7396\n",
            "Epoch 15/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4763 - accuracy: 0.7674 - val_loss: 0.5033 - val_accuracy: 0.7396\n",
            "Epoch 16/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4761 - accuracy: 0.7656 - val_loss: 0.5032 - val_accuracy: 0.7396\n",
            "Epoch 17/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4759 - accuracy: 0.7656 - val_loss: 0.5030 - val_accuracy: 0.7396\n",
            "Epoch 18/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4757 - accuracy: 0.7674 - val_loss: 0.5029 - val_accuracy: 0.7396\n",
            "Epoch 19/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4755 - accuracy: 0.7674 - val_loss: 0.5028 - val_accuracy: 0.7396\n",
            "Epoch 20/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4753 - accuracy: 0.7708 - val_loss: 0.5027 - val_accuracy: 0.7396\n",
            "Epoch 21/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4752 - accuracy: 0.7691 - val_loss: 0.5026 - val_accuracy: 0.7396\n",
            "Epoch 22/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4749 - accuracy: 0.7674 - val_loss: 0.5025 - val_accuracy: 0.7396\n",
            "Epoch 23/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4747 - accuracy: 0.7691 - val_loss: 0.5024 - val_accuracy: 0.7396\n",
            "Epoch 24/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4746 - accuracy: 0.7674 - val_loss: 0.5023 - val_accuracy: 0.7396\n",
            "Epoch 25/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4744 - accuracy: 0.7708 - val_loss: 0.5022 - val_accuracy: 0.7396\n",
            "Epoch 26/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4742 - accuracy: 0.7726 - val_loss: 0.5021 - val_accuracy: 0.7396\n",
            "Epoch 27/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4740 - accuracy: 0.7708 - val_loss: 0.5020 - val_accuracy: 0.7396\n",
            "Epoch 28/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4738 - accuracy: 0.7691 - val_loss: 0.5019 - val_accuracy: 0.7396\n",
            "Epoch 29/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4736 - accuracy: 0.7691 - val_loss: 0.5018 - val_accuracy: 0.7396\n",
            "Epoch 30/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4734 - accuracy: 0.7726 - val_loss: 0.5017 - val_accuracy: 0.7396\n",
            "Epoch 31/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4732 - accuracy: 0.7726 - val_loss: 0.5016 - val_accuracy: 0.7396\n",
            "Epoch 32/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4731 - accuracy: 0.7726 - val_loss: 0.5015 - val_accuracy: 0.7396\n",
            "Epoch 33/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4729 - accuracy: 0.7743 - val_loss: 0.5014 - val_accuracy: 0.7396\n",
            "Epoch 34/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.7743 - val_loss: 0.5013 - val_accuracy: 0.7396\n",
            "Epoch 35/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4725 - accuracy: 0.7726 - val_loss: 0.5012 - val_accuracy: 0.7396\n",
            "Epoch 36/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4723 - accuracy: 0.7708 - val_loss: 0.5011 - val_accuracy: 0.7448\n",
            "Epoch 37/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4721 - accuracy: 0.7726 - val_loss: 0.5010 - val_accuracy: 0.7448\n",
            "Epoch 38/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4720 - accuracy: 0.7726 - val_loss: 0.5009 - val_accuracy: 0.7448\n",
            "Epoch 39/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4718 - accuracy: 0.7726 - val_loss: 0.5008 - val_accuracy: 0.7448\n",
            "Epoch 40/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4716 - accuracy: 0.7743 - val_loss: 0.5007 - val_accuracy: 0.7448\n",
            "Epoch 41/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4715 - accuracy: 0.7743 - val_loss: 0.5006 - val_accuracy: 0.7448\n",
            "Epoch 42/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4713 - accuracy: 0.7743 - val_loss: 0.5005 - val_accuracy: 0.7448\n",
            "Epoch 43/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4711 - accuracy: 0.7743 - val_loss: 0.5004 - val_accuracy: 0.7448\n",
            "Epoch 44/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4709 - accuracy: 0.7760 - val_loss: 0.5004 - val_accuracy: 0.7448\n",
            "Epoch 45/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4708 - accuracy: 0.7760 - val_loss: 0.5003 - val_accuracy: 0.7448\n",
            "Epoch 46/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4706 - accuracy: 0.7760 - val_loss: 0.5002 - val_accuracy: 0.7448\n",
            "Epoch 47/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4705 - accuracy: 0.7760 - val_loss: 0.5001 - val_accuracy: 0.7448\n",
            "Epoch 48/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4703 - accuracy: 0.7760 - val_loss: 0.5000 - val_accuracy: 0.7448\n",
            "Epoch 49/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4701 - accuracy: 0.7760 - val_loss: 0.4999 - val_accuracy: 0.7448\n",
            "Epoch 50/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4700 - accuracy: 0.7760 - val_loss: 0.4998 - val_accuracy: 0.7448\n",
            "Epoch 51/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4698 - accuracy: 0.7760 - val_loss: 0.4998 - val_accuracy: 0.7448\n",
            "Epoch 52/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4697 - accuracy: 0.7760 - val_loss: 0.4997 - val_accuracy: 0.7448\n",
            "Epoch 53/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4695 - accuracy: 0.7760 - val_loss: 0.4996 - val_accuracy: 0.7396\n",
            "Epoch 54/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4693 - accuracy: 0.7760 - val_loss: 0.4995 - val_accuracy: 0.7448\n",
            "Epoch 55/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4692 - accuracy: 0.7760 - val_loss: 0.4994 - val_accuracy: 0.7448\n",
            "Epoch 56/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4690 - accuracy: 0.7760 - val_loss: 0.4994 - val_accuracy: 0.7448\n",
            "Epoch 57/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4689 - accuracy: 0.7760 - val_loss: 0.4993 - val_accuracy: 0.7448\n",
            "Epoch 58/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4688 - accuracy: 0.7760 - val_loss: 0.4992 - val_accuracy: 0.7448\n",
            "Epoch 59/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4686 - accuracy: 0.7743 - val_loss: 0.4991 - val_accuracy: 0.7448\n",
            "Epoch 60/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4684 - accuracy: 0.7760 - val_loss: 0.4991 - val_accuracy: 0.7448\n",
            "Epoch 61/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4683 - accuracy: 0.7726 - val_loss: 0.4990 - val_accuracy: 0.7448\n",
            "Epoch 62/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4681 - accuracy: 0.7743 - val_loss: 0.4989 - val_accuracy: 0.7448\n",
            "Epoch 63/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4680 - accuracy: 0.7726 - val_loss: 0.4988 - val_accuracy: 0.7448\n",
            "Epoch 64/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4679 - accuracy: 0.7726 - val_loss: 0.4988 - val_accuracy: 0.7448\n",
            "Epoch 65/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4677 - accuracy: 0.7708 - val_loss: 0.4987 - val_accuracy: 0.7500\n",
            "Epoch 66/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4676 - accuracy: 0.7691 - val_loss: 0.4986 - val_accuracy: 0.7500\n",
            "Epoch 67/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4674 - accuracy: 0.7726 - val_loss: 0.4986 - val_accuracy: 0.7500\n",
            "Epoch 68/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4673 - accuracy: 0.7708 - val_loss: 0.4985 - val_accuracy: 0.7500\n",
            "Epoch 69/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4671 - accuracy: 0.7708 - val_loss: 0.4984 - val_accuracy: 0.7448\n",
            "Epoch 70/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4670 - accuracy: 0.7726 - val_loss: 0.4983 - val_accuracy: 0.7448\n",
            "Epoch 71/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4669 - accuracy: 0.7726 - val_loss: 0.4983 - val_accuracy: 0.7448\n",
            "Epoch 72/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4667 - accuracy: 0.7726 - val_loss: 0.4982 - val_accuracy: 0.7448\n",
            "Epoch 73/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4666 - accuracy: 0.7726 - val_loss: 0.4981 - val_accuracy: 0.7448\n",
            "Epoch 74/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4665 - accuracy: 0.7726 - val_loss: 0.4981 - val_accuracy: 0.7448\n",
            "Epoch 75/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4663 - accuracy: 0.7743 - val_loss: 0.4980 - val_accuracy: 0.7448\n",
            "Epoch 76/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4662 - accuracy: 0.7743 - val_loss: 0.4979 - val_accuracy: 0.7448\n",
            "Epoch 77/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4660 - accuracy: 0.7743 - val_loss: 0.4979 - val_accuracy: 0.7448\n",
            "Epoch 78/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4659 - accuracy: 0.7743 - val_loss: 0.4978 - val_accuracy: 0.7448\n",
            "Epoch 79/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4658 - accuracy: 0.7743 - val_loss: 0.4977 - val_accuracy: 0.7448\n",
            "Epoch 80/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4656 - accuracy: 0.7760 - val_loss: 0.4977 - val_accuracy: 0.7448\n",
            "Epoch 81/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4655 - accuracy: 0.7760 - val_loss: 0.4976 - val_accuracy: 0.7448\n",
            "Epoch 82/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4654 - accuracy: 0.7760 - val_loss: 0.4976 - val_accuracy: 0.7448\n",
            "Epoch 83/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4653 - accuracy: 0.7760 - val_loss: 0.4975 - val_accuracy: 0.7448\n",
            "Epoch 84/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4651 - accuracy: 0.7760 - val_loss: 0.4974 - val_accuracy: 0.7448\n",
            "Epoch 85/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4650 - accuracy: 0.7760 - val_loss: 0.4974 - val_accuracy: 0.7448\n",
            "Epoch 86/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4649 - accuracy: 0.7760 - val_loss: 0.4973 - val_accuracy: 0.7448\n",
            "Epoch 87/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4648 - accuracy: 0.7760 - val_loss: 0.4972 - val_accuracy: 0.7448\n",
            "Epoch 88/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4646 - accuracy: 0.7778 - val_loss: 0.4972 - val_accuracy: 0.7448\n",
            "Epoch 89/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4645 - accuracy: 0.7795 - val_loss: 0.4971 - val_accuracy: 0.7448\n",
            "Epoch 90/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4644 - accuracy: 0.7830 - val_loss: 0.4971 - val_accuracy: 0.7448\n",
            "Epoch 91/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4643 - accuracy: 0.7830 - val_loss: 0.4970 - val_accuracy: 0.7448\n",
            "Epoch 92/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4641 - accuracy: 0.7830 - val_loss: 0.4969 - val_accuracy: 0.7448\n",
            "Epoch 93/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4640 - accuracy: 0.7830 - val_loss: 0.4969 - val_accuracy: 0.7448\n",
            "Epoch 94/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4639 - accuracy: 0.7830 - val_loss: 0.4968 - val_accuracy: 0.7448\n",
            "Epoch 95/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4638 - accuracy: 0.7830 - val_loss: 0.4967 - val_accuracy: 0.7448\n",
            "Epoch 96/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.7830 - val_loss: 0.4967 - val_accuracy: 0.7448\n",
            "Epoch 97/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4635 - accuracy: 0.7830 - val_loss: 0.4966 - val_accuracy: 0.7448\n",
            "Epoch 98/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4634 - accuracy: 0.7830 - val_loss: 0.4965 - val_accuracy: 0.7448\n",
            "Epoch 99/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4632 - accuracy: 0.7830 - val_loss: 0.4965 - val_accuracy: 0.7448\n",
            "Epoch 100/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4631 - accuracy: 0.7830 - val_loss: 0.4964 - val_accuracy: 0.7448\n",
            "Epoch 101/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4630 - accuracy: 0.7830 - val_loss: 0.4964 - val_accuracy: 0.7448\n",
            "Epoch 102/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4629 - accuracy: 0.7830 - val_loss: 0.4963 - val_accuracy: 0.7448\n",
            "Epoch 103/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4628 - accuracy: 0.7830 - val_loss: 0.4962 - val_accuracy: 0.7448\n",
            "Epoch 104/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4626 - accuracy: 0.7847 - val_loss: 0.4962 - val_accuracy: 0.7500\n",
            "Epoch 105/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4625 - accuracy: 0.7847 - val_loss: 0.4961 - val_accuracy: 0.7500\n",
            "Epoch 106/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4624 - accuracy: 0.7847 - val_loss: 0.4961 - val_accuracy: 0.7500\n",
            "Epoch 107/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4623 - accuracy: 0.7847 - val_loss: 0.4960 - val_accuracy: 0.7500\n",
            "Epoch 108/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4622 - accuracy: 0.7847 - val_loss: 0.4960 - val_accuracy: 0.7500\n",
            "Epoch 109/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4621 - accuracy: 0.7847 - val_loss: 0.4959 - val_accuracy: 0.7500\n",
            "Epoch 110/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4619 - accuracy: 0.7847 - val_loss: 0.4959 - val_accuracy: 0.7500\n",
            "Epoch 111/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4619 - accuracy: 0.7847 - val_loss: 0.4958 - val_accuracy: 0.7500\n",
            "Epoch 112/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4617 - accuracy: 0.7847 - val_loss: 0.4958 - val_accuracy: 0.7500\n",
            "Epoch 113/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4616 - accuracy: 0.7847 - val_loss: 0.4957 - val_accuracy: 0.7500\n",
            "Epoch 114/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4615 - accuracy: 0.7847 - val_loss: 0.4957 - val_accuracy: 0.7500\n",
            "Epoch 115/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4614 - accuracy: 0.7847 - val_loss: 0.4956 - val_accuracy: 0.7500\n",
            "Epoch 116/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4613 - accuracy: 0.7847 - val_loss: 0.4956 - val_accuracy: 0.7500\n",
            "Epoch 117/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4612 - accuracy: 0.7830 - val_loss: 0.4955 - val_accuracy: 0.7500\n",
            "Epoch 118/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4611 - accuracy: 0.7830 - val_loss: 0.4955 - val_accuracy: 0.7500\n",
            "Epoch 119/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4610 - accuracy: 0.7830 - val_loss: 0.4954 - val_accuracy: 0.7500\n",
            "Epoch 120/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4609 - accuracy: 0.7830 - val_loss: 0.4954 - val_accuracy: 0.7500\n",
            "Epoch 121/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4608 - accuracy: 0.7812 - val_loss: 0.4953 - val_accuracy: 0.7500\n",
            "Epoch 122/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4607 - accuracy: 0.7812 - val_loss: 0.4953 - val_accuracy: 0.7500\n",
            "Epoch 123/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4606 - accuracy: 0.7812 - val_loss: 0.4953 - val_accuracy: 0.7500\n",
            "Epoch 124/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4605 - accuracy: 0.7812 - val_loss: 0.4952 - val_accuracy: 0.7500\n",
            "Epoch 125/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4604 - accuracy: 0.7812 - val_loss: 0.4952 - val_accuracy: 0.7500\n",
            "Epoch 126/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4603 - accuracy: 0.7812 - val_loss: 0.4951 - val_accuracy: 0.7500\n",
            "Epoch 127/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4602 - accuracy: 0.7812 - val_loss: 0.4951 - val_accuracy: 0.7500\n",
            "Epoch 128/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4601 - accuracy: 0.7812 - val_loss: 0.4950 - val_accuracy: 0.7500\n",
            "Epoch 129/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4600 - accuracy: 0.7812 - val_loss: 0.4950 - val_accuracy: 0.7500\n",
            "Epoch 130/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4599 - accuracy: 0.7812 - val_loss: 0.4949 - val_accuracy: 0.7500\n",
            "Epoch 131/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4598 - accuracy: 0.7812 - val_loss: 0.4949 - val_accuracy: 0.7500\n",
            "Epoch 132/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4597 - accuracy: 0.7812 - val_loss: 0.4948 - val_accuracy: 0.7500\n",
            "Epoch 133/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4596 - accuracy: 0.7812 - val_loss: 0.4948 - val_accuracy: 0.7500\n",
            "Epoch 134/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.7830 - val_loss: 0.4948 - val_accuracy: 0.7500\n",
            "Epoch 135/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4593 - accuracy: 0.7795 - val_loss: 0.4947 - val_accuracy: 0.7500\n",
            "Epoch 136/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4593 - accuracy: 0.7812 - val_loss: 0.4947 - val_accuracy: 0.7500\n",
            "Epoch 137/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4592 - accuracy: 0.7812 - val_loss: 0.4947 - val_accuracy: 0.7500\n",
            "Epoch 138/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4591 - accuracy: 0.7830 - val_loss: 0.4946 - val_accuracy: 0.7500\n",
            "Epoch 139/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4946 - val_accuracy: 0.7552\n",
            "Epoch 140/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4945 - val_accuracy: 0.7552\n",
            "Epoch 141/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4945 - val_accuracy: 0.7552\n",
            "Epoch 142/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4945 - val_accuracy: 0.7552\n",
            "Epoch 143/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4944 - val_accuracy: 0.7552\n",
            "Epoch 144/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4585 - accuracy: 0.7865 - val_loss: 0.4944 - val_accuracy: 0.7552\n",
            "Epoch 145/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4584 - accuracy: 0.7847 - val_loss: 0.4944 - val_accuracy: 0.7552\n",
            "Epoch 146/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4583 - accuracy: 0.7865 - val_loss: 0.4943 - val_accuracy: 0.7552\n",
            "Epoch 147/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.7865 - val_loss: 0.4943 - val_accuracy: 0.7552\n",
            "Epoch 148/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4581 - accuracy: 0.7882 - val_loss: 0.4943 - val_accuracy: 0.7552\n",
            "Epoch 149/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4580 - accuracy: 0.7882 - val_loss: 0.4942 - val_accuracy: 0.7552\n",
            "Epoch 150/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.7882 - val_loss: 0.4942 - val_accuracy: 0.7552\n",
            "Epoch 151/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.7882 - val_loss: 0.4942 - val_accuracy: 0.7552\n",
            "Epoch 152/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4577 - accuracy: 0.7882 - val_loss: 0.4941 - val_accuracy: 0.7552\n",
            "Epoch 153/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4576 - accuracy: 0.7882 - val_loss: 0.4941 - val_accuracy: 0.7552\n",
            "Epoch 154/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4575 - accuracy: 0.7882 - val_loss: 0.4941 - val_accuracy: 0.7552\n",
            "Epoch 155/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4574 - accuracy: 0.7882 - val_loss: 0.4940 - val_accuracy: 0.7552\n",
            "Epoch 156/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4573 - accuracy: 0.7882 - val_loss: 0.4940 - val_accuracy: 0.7552\n",
            "Epoch 157/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4572 - accuracy: 0.7882 - val_loss: 0.4940 - val_accuracy: 0.7552\n",
            "Epoch 158/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4571 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7552\n",
            "Epoch 159/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4571 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7552\n",
            "Epoch 160/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7552\n",
            "Epoch 161/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4569 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7552\n",
            "Epoch 162/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4568 - accuracy: 0.7882 - val_loss: 0.4938 - val_accuracy: 0.7552\n",
            "Epoch 163/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4567 - accuracy: 0.7882 - val_loss: 0.4938 - val_accuracy: 0.7552\n",
            "Epoch 164/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4566 - accuracy: 0.7882 - val_loss: 0.4938 - val_accuracy: 0.7552\n",
            "Epoch 165/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4565 - accuracy: 0.7882 - val_loss: 0.4937 - val_accuracy: 0.7552\n",
            "Epoch 166/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.7882 - val_loss: 0.4937 - val_accuracy: 0.7552\n",
            "Epoch 167/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4563 - accuracy: 0.7882 - val_loss: 0.4937 - val_accuracy: 0.7552\n",
            "Epoch 168/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4563 - accuracy: 0.7882 - val_loss: 0.4936 - val_accuracy: 0.7552\n",
            "Epoch 169/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4562 - accuracy: 0.7882 - val_loss: 0.4936 - val_accuracy: 0.7552\n",
            "Epoch 170/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4561 - accuracy: 0.7882 - val_loss: 0.4936 - val_accuracy: 0.7604\n",
            "Epoch 171/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4560 - accuracy: 0.7882 - val_loss: 0.4935 - val_accuracy: 0.7604\n",
            "Epoch 172/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4559 - accuracy: 0.7882 - val_loss: 0.4935 - val_accuracy: 0.7604\n",
            "Epoch 173/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4558 - accuracy: 0.7882 - val_loss: 0.4935 - val_accuracy: 0.7604\n",
            "Epoch 174/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4557 - accuracy: 0.7882 - val_loss: 0.4935 - val_accuracy: 0.7604\n",
            "Epoch 175/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4556 - accuracy: 0.7882 - val_loss: 0.4934 - val_accuracy: 0.7604\n",
            "Epoch 176/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4555 - accuracy: 0.7882 - val_loss: 0.4934 - val_accuracy: 0.7604\n",
            "Epoch 177/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4554 - accuracy: 0.7882 - val_loss: 0.4934 - val_accuracy: 0.7604\n",
            "Epoch 178/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4553 - accuracy: 0.7882 - val_loss: 0.4933 - val_accuracy: 0.7604\n",
            "Epoch 179/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4553 - accuracy: 0.7865 - val_loss: 0.4933 - val_accuracy: 0.7604\n",
            "Epoch 180/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4552 - accuracy: 0.7865 - val_loss: 0.4933 - val_accuracy: 0.7604\n",
            "Epoch 181/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4551 - accuracy: 0.7882 - val_loss: 0.4933 - val_accuracy: 0.7604\n",
            "Epoch 182/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4550 - accuracy: 0.7882 - val_loss: 0.4932 - val_accuracy: 0.7604\n",
            "Epoch 183/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4549 - accuracy: 0.7865 - val_loss: 0.4932 - val_accuracy: 0.7604\n",
            "Epoch 184/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4548 - accuracy: 0.7882 - val_loss: 0.4932 - val_accuracy: 0.7604\n",
            "Epoch 185/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4547 - accuracy: 0.7882 - val_loss: 0.4931 - val_accuracy: 0.7604\n",
            "Epoch 186/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4546 - accuracy: 0.7865 - val_loss: 0.4931 - val_accuracy: 0.7604\n",
            "Epoch 187/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4545 - accuracy: 0.7882 - val_loss: 0.4931 - val_accuracy: 0.7604\n",
            "Epoch 188/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4545 - accuracy: 0.7847 - val_loss: 0.4931 - val_accuracy: 0.7604\n",
            "Epoch 189/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4544 - accuracy: 0.7882 - val_loss: 0.4930 - val_accuracy: 0.7604\n",
            "Epoch 190/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4543 - accuracy: 0.7865 - val_loss: 0.4930 - val_accuracy: 0.7604\n",
            "Epoch 191/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4542 - accuracy: 0.7865 - val_loss: 0.4930 - val_accuracy: 0.7604\n",
            "Epoch 192/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4541 - accuracy: 0.7847 - val_loss: 0.4930 - val_accuracy: 0.7604\n",
            "Epoch 193/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.7865 - val_loss: 0.4929 - val_accuracy: 0.7604\n",
            "Epoch 194/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.7847 - val_loss: 0.4929 - val_accuracy: 0.7604\n",
            "Epoch 195/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4539 - accuracy: 0.7847 - val_loss: 0.4929 - val_accuracy: 0.7604\n",
            "Epoch 196/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4538 - accuracy: 0.7865 - val_loss: 0.4929 - val_accuracy: 0.7604\n",
            "Epoch 197/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4537 - accuracy: 0.7847 - val_loss: 0.4929 - val_accuracy: 0.7604\n",
            "Epoch 198/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.7847 - val_loss: 0.4928 - val_accuracy: 0.7604\n",
            "Epoch 199/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4535 - accuracy: 0.7847 - val_loss: 0.4928 - val_accuracy: 0.7604\n",
            "Epoch 200/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4534 - accuracy: 0.7847 - val_loss: 0.4928 - val_accuracy: 0.7604\n",
            "Epoch 201/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4534 - accuracy: 0.7847 - val_loss: 0.4928 - val_accuracy: 0.7604\n",
            "Epoch 202/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4533 - accuracy: 0.7847 - val_loss: 0.4927 - val_accuracy: 0.7604\n",
            "Epoch 203/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4532 - accuracy: 0.7847 - val_loss: 0.4927 - val_accuracy: 0.7604\n",
            "Epoch 204/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4531 - accuracy: 0.7847 - val_loss: 0.4927 - val_accuracy: 0.7604\n",
            "Epoch 205/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4530 - accuracy: 0.7847 - val_loss: 0.4927 - val_accuracy: 0.7604\n",
            "Epoch 206/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4529 - accuracy: 0.7847 - val_loss: 0.4927 - val_accuracy: 0.7604\n",
            "Epoch 207/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4529 - accuracy: 0.7847 - val_loss: 0.4926 - val_accuracy: 0.7604\n",
            "Epoch 208/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4528 - accuracy: 0.7847 - val_loss: 0.4926 - val_accuracy: 0.7604\n",
            "Epoch 209/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4527 - accuracy: 0.7847 - val_loss: 0.4926 - val_accuracy: 0.7604\n",
            "Epoch 210/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4526 - accuracy: 0.7830 - val_loss: 0.4926 - val_accuracy: 0.7604\n",
            "Epoch 211/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4525 - accuracy: 0.7830 - val_loss: 0.4926 - val_accuracy: 0.7604\n",
            "Epoch 212/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4524 - accuracy: 0.7830 - val_loss: 0.4925 - val_accuracy: 0.7604\n",
            "Epoch 213/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4524 - accuracy: 0.7830 - val_loss: 0.4925 - val_accuracy: 0.7604\n",
            "Epoch 214/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4523 - accuracy: 0.7830 - val_loss: 0.4925 - val_accuracy: 0.7604\n",
            "Epoch 215/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4522 - accuracy: 0.7830 - val_loss: 0.4925 - val_accuracy: 0.7604\n",
            "Epoch 216/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4521 - accuracy: 0.7830 - val_loss: 0.4925 - val_accuracy: 0.7552\n",
            "Epoch 217/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4521 - accuracy: 0.7830 - val_loss: 0.4925 - val_accuracy: 0.7552\n",
            "Epoch 218/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4520 - accuracy: 0.7830 - val_loss: 0.4925 - val_accuracy: 0.7552\n",
            "Epoch 219/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4519 - accuracy: 0.7830 - val_loss: 0.4924 - val_accuracy: 0.7552\n",
            "Epoch 220/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4518 - accuracy: 0.7830 - val_loss: 0.4924 - val_accuracy: 0.7552\n",
            "Epoch 221/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4517 - accuracy: 0.7830 - val_loss: 0.4924 - val_accuracy: 0.7552\n",
            "Epoch 222/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4517 - accuracy: 0.7830 - val_loss: 0.4924 - val_accuracy: 0.7552\n",
            "Epoch 223/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4516 - accuracy: 0.7830 - val_loss: 0.4924 - val_accuracy: 0.7552\n",
            "Epoch 224/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4515 - accuracy: 0.7830 - val_loss: 0.4924 - val_accuracy: 0.7552\n",
            "Epoch 225/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4514 - accuracy: 0.7830 - val_loss: 0.4923 - val_accuracy: 0.7552\n",
            "Epoch 226/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4514 - accuracy: 0.7830 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
            "Epoch 227/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4513 - accuracy: 0.7830 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
            "Epoch 228/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4512 - accuracy: 0.7830 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
            "Epoch 229/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4511 - accuracy: 0.7830 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
            "Epoch 230/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4510 - accuracy: 0.7830 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
            "Epoch 231/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4510 - accuracy: 0.7830 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
            "Epoch 232/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4509 - accuracy: 0.7830 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
            "Epoch 233/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4508 - accuracy: 0.7830 - val_loss: 0.4922 - val_accuracy: 0.7604\n",
            "Epoch 234/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4507 - accuracy: 0.7830 - val_loss: 0.4922 - val_accuracy: 0.7604\n",
            "Epoch 235/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4506 - accuracy: 0.7830 - val_loss: 0.4922 - val_accuracy: 0.7604\n",
            "Epoch 236/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4506 - accuracy: 0.7847 - val_loss: 0.4922 - val_accuracy: 0.7604\n",
            "Epoch 237/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4505 - accuracy: 0.7830 - val_loss: 0.4922 - val_accuracy: 0.7604\n",
            "Epoch 238/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4504 - accuracy: 0.7847 - val_loss: 0.4922 - val_accuracy: 0.7656\n",
            "Epoch 239/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.7830 - val_loss: 0.4922 - val_accuracy: 0.7656\n",
            "Epoch 240/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.7865 - val_loss: 0.4922 - val_accuracy: 0.7656\n",
            "Epoch 241/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4502 - accuracy: 0.7865 - val_loss: 0.4922 - val_accuracy: 0.7656\n",
            "Epoch 242/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4501 - accuracy: 0.7865 - val_loss: 0.4922 - val_accuracy: 0.7604\n",
            "Epoch 243/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4501 - accuracy: 0.7865 - val_loss: 0.4921 - val_accuracy: 0.7604\n",
            "Epoch 244/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4500 - accuracy: 0.7865 - val_loss: 0.4921 - val_accuracy: 0.7604\n",
            "Epoch 245/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.7847 - val_loss: 0.4921 - val_accuracy: 0.7604\n",
            "Epoch 246/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4498 - accuracy: 0.7847 - val_loss: 0.4921 - val_accuracy: 0.7604\n",
            "Epoch 247/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4498 - accuracy: 0.7847 - val_loss: 0.4921 - val_accuracy: 0.7604\n",
            "Epoch 248/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4497 - accuracy: 0.7847 - val_loss: 0.4921 - val_accuracy: 0.7604\n",
            "Epoch 249/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4496 - accuracy: 0.7847 - val_loss: 0.4921 - val_accuracy: 0.7604\n",
            "Epoch 250/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4496 - accuracy: 0.7847 - val_loss: 0.4921 - val_accuracy: 0.7604\n",
            "Epoch 251/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4495 - accuracy: 0.7847 - val_loss: 0.4921 - val_accuracy: 0.7604\n",
            "Epoch 252/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4494 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 253/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4493 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 254/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4493 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 255/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4492 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 256/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4492 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 257/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4491 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 258/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4490 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 259/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4489 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 260/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4489 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 261/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4488 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 262/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4487 - accuracy: 0.7847 - val_loss: 0.4920 - val_accuracy: 0.7604\n",
            "Epoch 263/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4486 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 264/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4486 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 265/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4485 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 266/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4485 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 267/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4484 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 268/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4483 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 269/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4483 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 270/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4482 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 271/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4481 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 272/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4480 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 273/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4479 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 274/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4479 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 275/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4478 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 276/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4477 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 277/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4477 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 278/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4476 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 279/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4476 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 280/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4475 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 281/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4474 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 282/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4473 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 283/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4472 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7604\n",
            "Epoch 284/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4472 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 285/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4471 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 286/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4470 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 287/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4470 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 288/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4469 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 289/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4468 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 290/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4468 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 291/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4467 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 292/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4466 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 293/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4465 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 294/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4465 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 295/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4464 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 296/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4463 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 297/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4463 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 298/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4462 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 299/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4461 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 300/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4461 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 301/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4460 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 302/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4459 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 303/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4459 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 304/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4458 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 305/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4457 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 306/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4457 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 307/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4456 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 308/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4456 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 309/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4454 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 310/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4454 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 311/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4453 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 312/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4453 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 313/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4452 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 314/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4451 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 315/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4451 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 316/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4450 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 317/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4450 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 318/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4449 - accuracy: 0.7899 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 319/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4449 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 320/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4448 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 321/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4447 - accuracy: 0.7899 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 322/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4447 - accuracy: 0.7899 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 323/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4446 - accuracy: 0.7899 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 324/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4445 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 325/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4445 - accuracy: 0.7865 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 326/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4444 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 327/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4444 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
            "Epoch 328/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4443 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7656\n",
            "Epoch 329/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4442 - accuracy: 0.7882 - val_loss: 0.4918 - val_accuracy: 0.7656\n",
            "Epoch 330/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7656\n",
            "Epoch 331/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7656\n",
            "Epoch 332/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7656\n",
            "Epoch 333/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7656\n",
            "Epoch 334/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7656\n",
            "Epoch 335/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4439 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7656\n",
            "Epoch 336/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4438 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7656\n",
            "Epoch 337/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4438 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7656\n",
            "Epoch 338/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4438 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 339/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4437 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 340/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4437 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 341/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4436 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 342/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4436 - accuracy: 0.7865 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 343/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4435 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 344/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4435 - accuracy: 0.7899 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 345/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4434 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 346/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4434 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 347/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4433 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 348/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4433 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 349/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4432 - accuracy: 0.7882 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
            "Epoch 350/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4432 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 351/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4431 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 352/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4431 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 353/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4430 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 354/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4430 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 355/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4429 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 356/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4429 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 357/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4428 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 358/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4428 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 359/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4427 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 360/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4427 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 361/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4426 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 362/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4426 - accuracy: 0.7882 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 363/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4425 - accuracy: 0.7899 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 364/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4425 - accuracy: 0.7899 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 365/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4425 - accuracy: 0.7899 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 366/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4424 - accuracy: 0.7899 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 367/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4423 - accuracy: 0.7899 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
            "Epoch 368/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4423 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 369/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4423 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 370/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4422 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 371/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 372/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4421 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 373/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4421 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 374/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4421 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 375/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4420 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7760\n",
            "Epoch 376/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4420 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7760\n",
            "Epoch 377/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4419 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 378/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4419 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 379/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4418 - accuracy: 0.7899 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
            "Epoch 380/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4418 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 381/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4417 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 382/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4417 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 383/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4417 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 384/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4416 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 385/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4416 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 386/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4415 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 387/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4415 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 388/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4415 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 389/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4414 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 390/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4413 - accuracy: 0.7899 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
            "Epoch 391/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4413 - accuracy: 0.7899 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 392/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4413 - accuracy: 0.7899 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 393/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4412 - accuracy: 0.7899 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 394/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4412 - accuracy: 0.7899 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 395/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4412 - accuracy: 0.7899 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 396/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4411 - accuracy: 0.7882 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 397/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4411 - accuracy: 0.7899 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 398/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4410 - accuracy: 0.7882 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 399/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4410 - accuracy: 0.7882 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 400/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4410 - accuracy: 0.7882 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
            "Epoch 401/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4409 - accuracy: 0.7899 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
            "Epoch 402/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4409 - accuracy: 0.7882 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
            "Epoch 403/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4408 - accuracy: 0.7882 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
            "Epoch 404/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.7882 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
            "Epoch 405/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4407 - accuracy: 0.7899 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
            "Epoch 406/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4407 - accuracy: 0.7882 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
            "Epoch 407/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4407 - accuracy: 0.7899 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
            "Epoch 408/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4406 - accuracy: 0.7899 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
            "Epoch 409/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4406 - accuracy: 0.7899 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
            "Epoch 410/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4406 - accuracy: 0.7899 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 411/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4405 - accuracy: 0.7899 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 412/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4405 - accuracy: 0.7899 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 413/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4404 - accuracy: 0.7917 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 414/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4404 - accuracy: 0.7899 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 415/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4404 - accuracy: 0.7899 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 416/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4403 - accuracy: 0.7917 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 417/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4403 - accuracy: 0.7917 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 418/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4403 - accuracy: 0.7917 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 419/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4402 - accuracy: 0.7917 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 420/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4402 - accuracy: 0.7917 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
            "Epoch 421/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4401 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
            "Epoch 422/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4401 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
            "Epoch 423/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
            "Epoch 424/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
            "Epoch 425/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
            "Epoch 426/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
            "Epoch 427/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4399 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
            "Epoch 428/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4398 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7760\n",
            "Epoch 429/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4398 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7760\n",
            "Epoch 430/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4397 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7760\n",
            "Epoch 431/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4397 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7760\n",
            "Epoch 432/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4397 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7760\n",
            "Epoch 433/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4396 - accuracy: 0.7917 - val_loss: 0.4926 - val_accuracy: 0.7760\n",
            "Epoch 434/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4396 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7760\n",
            "Epoch 435/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4395 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7760\n",
            "Epoch 436/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4395 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7760\n",
            "Epoch 437/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4394 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 438/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4394 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 439/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4394 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 440/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4393 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 441/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4393 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 442/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4392 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 443/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4392 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 444/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4392 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 445/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4391 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 446/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4391 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 447/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4390 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 448/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4390 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 449/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4390 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 450/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4389 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 451/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4389 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 452/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4389 - accuracy: 0.7917 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 453/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4388 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 454/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4388 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 455/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4387 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 456/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4387 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 457/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4386 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 458/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4386 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 459/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4385 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 460/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4385 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 461/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4384 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 462/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4384 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 463/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4384 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 464/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4383 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 465/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4383 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 466/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4382 - accuracy: 0.7917 - val_loss: 0.4928 - val_accuracy: 0.7812\n",
            "Epoch 467/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4382 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 468/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4381 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 469/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4381 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 470/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4380 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 471/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4380 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 472/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4379 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 473/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4379 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 474/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4378 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 475/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4378 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 476/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4377 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 477/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4377 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 478/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4377 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 479/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4376 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 480/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4376 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 481/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4375 - accuracy: 0.7917 - val_loss: 0.4929 - val_accuracy: 0.7812\n",
            "Epoch 482/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4374 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 483/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4374 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 484/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4374 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 485/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4373 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 486/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4373 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 487/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4372 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 488/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4372 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 489/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4372 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 490/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4371 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 491/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4371 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 492/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4370 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 493/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4370 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 494/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4369 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 495/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4369 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 496/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4368 - accuracy: 0.7917 - val_loss: 0.4930 - val_accuracy: 0.7812\n",
            "Epoch 497/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4368 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 498/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4367 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 499/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4367 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 500/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4367 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 501/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4366 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 502/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4366 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 503/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4365 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 504/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4365 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 505/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4365 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 506/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4364 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 507/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4364 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 508/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4363 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 509/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4363 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 510/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4362 - accuracy: 0.7899 - val_loss: 0.4931 - val_accuracy: 0.7812\n",
            "Epoch 511/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4362 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 512/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 513/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 514/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 515/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4360 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 516/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4360 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 517/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4359 - accuracy: 0.7917 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 518/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4359 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 519/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4358 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 520/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4358 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 521/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4358 - accuracy: 0.7899 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
            "Epoch 522/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.7899 - val_loss: 0.4933 - val_accuracy: 0.7812\n",
            "Epoch 523/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4357 - accuracy: 0.7899 - val_loss: 0.4933 - val_accuracy: 0.7812\n",
            "Epoch 524/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4356 - accuracy: 0.7899 - val_loss: 0.4933 - val_accuracy: 0.7812\n",
            "Epoch 525/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4356 - accuracy: 0.7899 - val_loss: 0.4933 - val_accuracy: 0.7812\n",
            "Epoch 526/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4356 - accuracy: 0.7899 - val_loss: 0.4933 - val_accuracy: 0.7812\n",
            "Epoch 527/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4355 - accuracy: 0.7917 - val_loss: 0.4933 - val_accuracy: 0.7812\n",
            "Epoch 528/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4355 - accuracy: 0.7934 - val_loss: 0.4933 - val_accuracy: 0.7812\n",
            "Epoch 529/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4354 - accuracy: 0.7934 - val_loss: 0.4933 - val_accuracy: 0.7812\n",
            "Epoch 530/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4354 - accuracy: 0.7917 - val_loss: 0.4933 - val_accuracy: 0.7812\n",
            "Epoch 531/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4353 - accuracy: 0.7917 - val_loss: 0.4934 - val_accuracy: 0.7812\n",
            "Epoch 532/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4353 - accuracy: 0.7917 - val_loss: 0.4934 - val_accuracy: 0.7812\n",
            "Epoch 533/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4353 - accuracy: 0.7917 - val_loss: 0.4934 - val_accuracy: 0.7812\n",
            "Epoch 534/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.7934 - val_loss: 0.4934 - val_accuracy: 0.7812\n",
            "Epoch 535/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4351 - accuracy: 0.7917 - val_loss: 0.4934 - val_accuracy: 0.7812\n",
            "Epoch 536/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.7917 - val_loss: 0.4934 - val_accuracy: 0.7812\n",
            "Epoch 537/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.7917 - val_loss: 0.4934 - val_accuracy: 0.7812\n",
            "Epoch 538/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4350 - accuracy: 0.7917 - val_loss: 0.4934 - val_accuracy: 0.7812\n",
            "Epoch 539/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4350 - accuracy: 0.7934 - val_loss: 0.4934 - val_accuracy: 0.7812\n",
            "Epoch 540/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4349 - accuracy: 0.7934 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 541/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4349 - accuracy: 0.7934 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 542/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4348 - accuracy: 0.7934 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 543/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4348 - accuracy: 0.7934 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 544/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4347 - accuracy: 0.7934 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 545/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4347 - accuracy: 0.7934 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 546/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4347 - accuracy: 0.7934 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 547/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4346 - accuracy: 0.7917 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 548/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4346 - accuracy: 0.7917 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 549/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4346 - accuracy: 0.7917 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 550/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4345 - accuracy: 0.7917 - val_loss: 0.4935 - val_accuracy: 0.7812\n",
            "Epoch 551/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4345 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 552/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4344 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 553/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4344 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 554/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4343 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 555/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4343 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 556/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4343 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 557/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4342 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 558/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4342 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 559/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4341 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 560/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4341 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 561/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4341 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 562/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4340 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 563/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4340 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 564/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 565/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4339 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 566/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4338 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 567/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4338 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 568/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4338 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 569/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4337 - accuracy: 0.7917 - val_loss: 0.4936 - val_accuracy: 0.7812\n",
            "Epoch 570/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4337 - accuracy: 0.7917 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 571/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4336 - accuracy: 0.7917 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 572/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4336 - accuracy: 0.7899 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 573/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4335 - accuracy: 0.7882 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 574/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4335 - accuracy: 0.7899 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 575/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4334 - accuracy: 0.7899 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 576/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4334 - accuracy: 0.7899 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 577/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4334 - accuracy: 0.7882 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 578/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4333 - accuracy: 0.7899 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 579/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4333 - accuracy: 0.7899 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 580/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4332 - accuracy: 0.7882 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 581/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4332 - accuracy: 0.7899 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 582/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4331 - accuracy: 0.7899 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 583/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4331 - accuracy: 0.7882 - val_loss: 0.4937 - val_accuracy: 0.7812\n",
            "Epoch 584/1000\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4331 - accuracy: 0.7865 - val_loss: 0.4937 - val_accuracy: 0.7865\n",
            "Epoch 585/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4330 - accuracy: 0.7882 - val_loss: 0.4937 - val_accuracy: 0.7865\n",
            "Epoch 586/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4330 - accuracy: 0.7865 - val_loss: 0.4937 - val_accuracy: 0.7865\n",
            "Epoch 587/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4329 - accuracy: 0.7882 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 588/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4329 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 589/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4329 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 590/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4328 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 591/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4328 - accuracy: 0.7882 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 592/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4327 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 593/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4327 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 594/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4327 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 595/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4326 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 596/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4326 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 597/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4325 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 598/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4325 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 599/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4325 - accuracy: 0.7865 - val_loss: 0.4938 - val_accuracy: 0.7865\n",
            "Epoch 600/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4324 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 601/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4324 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 602/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4324 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 603/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4323 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 604/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4323 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 605/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4323 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 606/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4322 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 607/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4321 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 608/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4321 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 609/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4321 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 610/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 611/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 612/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 613/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4319 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 614/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4319 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 615/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4318 - accuracy: 0.7865 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 616/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4318 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 617/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4317 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 618/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.7882 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 619/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4317 - accuracy: 0.7899 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 620/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4317 - accuracy: 0.7899 - val_loss: 0.4939 - val_accuracy: 0.7865\n",
            "Epoch 621/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4316 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 622/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4316 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 623/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4315 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 624/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4315 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 625/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4315 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 626/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 627/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4314 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 628/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4313 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 629/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 630/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4312 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 631/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4312 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 632/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 633/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4311 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 634/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4311 - accuracy: 0.7899 - val_loss: 0.4940 - val_accuracy: 0.7865\n",
            "Epoch 635/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4310 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 636/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4310 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 637/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4309 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 638/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4309 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 639/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4309 - accuracy: 0.7917 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 640/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4308 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 641/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4308 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 642/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4307 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 643/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4307 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 644/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4306 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 645/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4306 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 646/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4305 - accuracy: 0.7917 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 647/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4305 - accuracy: 0.7917 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 648/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4304 - accuracy: 0.7917 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 649/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4304 - accuracy: 0.7899 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 650/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4304 - accuracy: 0.7917 - val_loss: 0.4941 - val_accuracy: 0.7865\n",
            "Epoch 651/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4303 - accuracy: 0.7917 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 652/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.7917 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 653/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4302 - accuracy: 0.7917 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 654/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.7934 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 655/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.7934 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 656/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4301 - accuracy: 0.7917 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 657/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4301 - accuracy: 0.7934 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 658/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.7934 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 659/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4300 - accuracy: 0.7934 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 660/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4299 - accuracy: 0.7934 - val_loss: 0.4942 - val_accuracy: 0.7865\n",
            "Epoch 661/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4299 - accuracy: 0.7951 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 662/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4298 - accuracy: 0.7934 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 663/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4298 - accuracy: 0.7934 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 664/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4298 - accuracy: 0.7951 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 665/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4297 - accuracy: 0.7934 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 666/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4297 - accuracy: 0.7934 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 667/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4296 - accuracy: 0.7951 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 668/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4296 - accuracy: 0.7951 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 669/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4295 - accuracy: 0.7934 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 670/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4295 - accuracy: 0.7934 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 671/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.7934 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 672/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.7951 - val_loss: 0.4943 - val_accuracy: 0.7865\n",
            "Epoch 673/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4294 - accuracy: 0.7951 - val_loss: 0.4944 - val_accuracy: 0.7865\n",
            "Epoch 674/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4293 - accuracy: 0.7951 - val_loss: 0.4944 - val_accuracy: 0.7865\n",
            "Epoch 675/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.7951 - val_loss: 0.4944 - val_accuracy: 0.7865\n",
            "Epoch 676/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.7934 - val_loss: 0.4944 - val_accuracy: 0.7865\n",
            "Epoch 677/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4292 - accuracy: 0.7951 - val_loss: 0.4944 - val_accuracy: 0.7917\n",
            "Epoch 678/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4292 - accuracy: 0.7951 - val_loss: 0.4944 - val_accuracy: 0.7917\n",
            "Epoch 679/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4291 - accuracy: 0.7934 - val_loss: 0.4944 - val_accuracy: 0.7917\n",
            "Epoch 680/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4291 - accuracy: 0.7934 - val_loss: 0.4944 - val_accuracy: 0.7917\n",
            "Epoch 681/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.7951 - val_loss: 0.4944 - val_accuracy: 0.7917\n",
            "Epoch 682/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4290 - accuracy: 0.7951 - val_loss: 0.4944 - val_accuracy: 0.7917\n",
            "Epoch 683/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.7951 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 684/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4289 - accuracy: 0.7934 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 685/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4289 - accuracy: 0.7951 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 686/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.7934 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 687/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4288 - accuracy: 0.7951 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 688/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4288 - accuracy: 0.7969 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 689/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4287 - accuracy: 0.7969 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 690/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4287 - accuracy: 0.7969 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 691/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4286 - accuracy: 0.7969 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 692/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4286 - accuracy: 0.7969 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 693/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4285 - accuracy: 0.7969 - val_loss: 0.4945 - val_accuracy: 0.7917\n",
            "Epoch 694/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4285 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 695/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4285 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 696/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4284 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 697/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4284 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 698/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 699/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 700/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 701/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4282 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 702/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4282 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 703/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4282 - accuracy: 0.7969 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
            "Epoch 704/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4281 - accuracy: 0.7969 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 705/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4281 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 706/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4280 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 707/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4280 - accuracy: 0.7969 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 708/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4280 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 709/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4279 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 710/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4279 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 711/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4278 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 712/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4278 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 713/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4277 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 714/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4277 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 715/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4277 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 716/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.7986 - val_loss: 0.4947 - val_accuracy: 0.7917\n",
            "Epoch 717/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4276 - accuracy: 0.7986 - val_loss: 0.4948 - val_accuracy: 0.7917\n",
            "Epoch 718/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.7986 - val_loss: 0.4948 - val_accuracy: 0.7917\n",
            "Epoch 719/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4275 - accuracy: 0.7986 - val_loss: 0.4948 - val_accuracy: 0.7917\n",
            "Epoch 720/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4275 - accuracy: 0.7986 - val_loss: 0.4948 - val_accuracy: 0.7917\n",
            "Epoch 721/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4275 - accuracy: 0.7986 - val_loss: 0.4948 - val_accuracy: 0.7917\n",
            "Epoch 722/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4274 - accuracy: 0.7986 - val_loss: 0.4948 - val_accuracy: 0.7917\n",
            "Epoch 723/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4274 - accuracy: 0.7986 - val_loss: 0.4948 - val_accuracy: 0.7917\n",
            "Epoch 724/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4273 - accuracy: 0.7986 - val_loss: 0.4948 - val_accuracy: 0.7917\n",
            "Epoch 725/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4273 - accuracy: 0.7986 - val_loss: 0.4948 - val_accuracy: 0.7917\n",
            "Epoch 726/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4273 - accuracy: 0.7986 - val_loss: 0.4949 - val_accuracy: 0.7917\n",
            "Epoch 727/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4272 - accuracy: 0.7986 - val_loss: 0.4949 - val_accuracy: 0.7917\n",
            "Epoch 728/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4272 - accuracy: 0.7986 - val_loss: 0.4949 - val_accuracy: 0.7917\n",
            "Epoch 729/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4272 - accuracy: 0.7986 - val_loss: 0.4949 - val_accuracy: 0.7917\n",
            "Epoch 730/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4271 - accuracy: 0.7986 - val_loss: 0.4949 - val_accuracy: 0.7917\n",
            "Epoch 731/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4271 - accuracy: 0.7986 - val_loss: 0.4949 - val_accuracy: 0.7917\n",
            "Epoch 732/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4271 - accuracy: 0.7986 - val_loss: 0.4949 - val_accuracy: 0.7917\n",
            "Epoch 733/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4270 - accuracy: 0.7986 - val_loss: 0.4949 - val_accuracy: 0.7917\n",
            "Epoch 734/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4270 - accuracy: 0.7986 - val_loss: 0.4949 - val_accuracy: 0.7917\n",
            "Epoch 735/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4269 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 736/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4269 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 737/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4268 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 738/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4268 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 739/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4268 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 740/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4268 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 741/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4267 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 742/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4267 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 743/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4266 - accuracy: 0.8003 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 744/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4266 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 745/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4266 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 746/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4265 - accuracy: 0.7986 - val_loss: 0.4950 - val_accuracy: 0.7917\n",
            "Epoch 747/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4265 - accuracy: 0.8003 - val_loss: 0.4951 - val_accuracy: 0.7917\n",
            "Epoch 748/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4264 - accuracy: 0.7986 - val_loss: 0.4951 - val_accuracy: 0.7917\n",
            "Epoch 749/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4264 - accuracy: 0.7986 - val_loss: 0.4951 - val_accuracy: 0.7917\n",
            "Epoch 750/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4264 - accuracy: 0.7986 - val_loss: 0.4951 - val_accuracy: 0.7917\n",
            "Epoch 751/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4263 - accuracy: 0.7986 - val_loss: 0.4951 - val_accuracy: 0.7917\n",
            "Epoch 752/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4263 - accuracy: 0.7986 - val_loss: 0.4951 - val_accuracy: 0.7917\n",
            "Epoch 753/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4262 - accuracy: 0.7986 - val_loss: 0.4951 - val_accuracy: 0.7917\n",
            "Epoch 754/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4262 - accuracy: 0.7986 - val_loss: 0.4951 - val_accuracy: 0.7917\n",
            "Epoch 755/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4262 - accuracy: 0.8003 - val_loss: 0.4952 - val_accuracy: 0.7917\n",
            "Epoch 756/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4261 - accuracy: 0.7986 - val_loss: 0.4952 - val_accuracy: 0.7917\n",
            "Epoch 757/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4261 - accuracy: 0.8003 - val_loss: 0.4952 - val_accuracy: 0.7917\n",
            "Epoch 758/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4260 - accuracy: 0.8003 - val_loss: 0.4952 - val_accuracy: 0.7917\n",
            "Epoch 759/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4260 - accuracy: 0.7986 - val_loss: 0.4952 - val_accuracy: 0.7917\n",
            "Epoch 760/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4259 - accuracy: 0.8021 - val_loss: 0.4952 - val_accuracy: 0.7917\n",
            "Epoch 761/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4259 - accuracy: 0.8021 - val_loss: 0.4952 - val_accuracy: 0.7917\n",
            "Epoch 762/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4258 - accuracy: 0.8003 - val_loss: 0.4952 - val_accuracy: 0.7917\n",
            "Epoch 763/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4258 - accuracy: 0.8021 - val_loss: 0.4953 - val_accuracy: 0.7917\n",
            "Epoch 764/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4258 - accuracy: 0.8021 - val_loss: 0.4953 - val_accuracy: 0.7917\n",
            "Epoch 765/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4258 - accuracy: 0.8021 - val_loss: 0.4953 - val_accuracy: 0.7917\n",
            "Epoch 766/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4257 - accuracy: 0.8021 - val_loss: 0.4953 - val_accuracy: 0.7917\n",
            "Epoch 767/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4256 - accuracy: 0.8038 - val_loss: 0.4953 - val_accuracy: 0.7917\n",
            "Epoch 768/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4256 - accuracy: 0.8021 - val_loss: 0.4953 - val_accuracy: 0.7917\n",
            "Epoch 769/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4256 - accuracy: 0.8056 - val_loss: 0.4953 - val_accuracy: 0.7917\n",
            "Epoch 770/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4255 - accuracy: 0.8021 - val_loss: 0.4954 - val_accuracy: 0.7917\n",
            "Epoch 771/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4255 - accuracy: 0.8021 - val_loss: 0.4954 - val_accuracy: 0.7917\n",
            "Epoch 772/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4254 - accuracy: 0.8038 - val_loss: 0.4954 - val_accuracy: 0.7917\n",
            "Epoch 773/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4255 - accuracy: 0.8038 - val_loss: 0.4954 - val_accuracy: 0.7917\n",
            "Epoch 774/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4254 - accuracy: 0.8038 - val_loss: 0.4954 - val_accuracy: 0.7917\n",
            "Epoch 775/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4254 - accuracy: 0.8038 - val_loss: 0.4954 - val_accuracy: 0.7917\n",
            "Epoch 776/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4253 - accuracy: 0.8038 - val_loss: 0.4954 - val_accuracy: 0.7917\n",
            "Epoch 777/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4253 - accuracy: 0.8038 - val_loss: 0.4954 - val_accuracy: 0.7917\n",
            "Epoch 778/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4252 - accuracy: 0.8038 - val_loss: 0.4955 - val_accuracy: 0.7917\n",
            "Epoch 779/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4252 - accuracy: 0.8038 - val_loss: 0.4955 - val_accuracy: 0.7917\n",
            "Epoch 780/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4252 - accuracy: 0.8038 - val_loss: 0.4955 - val_accuracy: 0.7917\n",
            "Epoch 781/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4251 - accuracy: 0.8038 - val_loss: 0.4955 - val_accuracy: 0.7917\n",
            "Epoch 782/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4251 - accuracy: 0.8038 - val_loss: 0.4955 - val_accuracy: 0.7917\n",
            "Epoch 783/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4250 - accuracy: 0.8038 - val_loss: 0.4955 - val_accuracy: 0.7917\n",
            "Epoch 784/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4250 - accuracy: 0.8038 - val_loss: 0.4955 - val_accuracy: 0.7917\n",
            "Epoch 785/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4250 - accuracy: 0.8038 - val_loss: 0.4955 - val_accuracy: 0.7917\n",
            "Epoch 786/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4250 - accuracy: 0.8038 - val_loss: 0.4955 - val_accuracy: 0.7917\n",
            "Epoch 787/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4249 - accuracy: 0.8038 - val_loss: 0.4956 - val_accuracy: 0.7917\n",
            "Epoch 788/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4249 - accuracy: 0.8038 - val_loss: 0.4956 - val_accuracy: 0.7917\n",
            "Epoch 789/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4248 - accuracy: 0.8056 - val_loss: 0.4956 - val_accuracy: 0.7917\n",
            "Epoch 790/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4248 - accuracy: 0.8038 - val_loss: 0.4956 - val_accuracy: 0.7917\n",
            "Epoch 791/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4248 - accuracy: 0.8038 - val_loss: 0.4956 - val_accuracy: 0.7917\n",
            "Epoch 792/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4247 - accuracy: 0.8038 - val_loss: 0.4956 - val_accuracy: 0.7917\n",
            "Epoch 793/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4247 - accuracy: 0.8056 - val_loss: 0.4956 - val_accuracy: 0.7917\n",
            "Epoch 794/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4247 - accuracy: 0.8038 - val_loss: 0.4956 - val_accuracy: 0.7917\n",
            "Epoch 795/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4246 - accuracy: 0.8038 - val_loss: 0.4957 - val_accuracy: 0.7917\n",
            "Epoch 796/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4246 - accuracy: 0.8038 - val_loss: 0.4957 - val_accuracy: 0.7917\n",
            "Epoch 797/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4246 - accuracy: 0.8038 - val_loss: 0.4957 - val_accuracy: 0.7917\n",
            "Epoch 798/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4245 - accuracy: 0.8038 - val_loss: 0.4957 - val_accuracy: 0.7917\n",
            "Epoch 799/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4245 - accuracy: 0.8038 - val_loss: 0.4957 - val_accuracy: 0.7917\n",
            "Epoch 800/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4245 - accuracy: 0.8038 - val_loss: 0.4957 - val_accuracy: 0.7917\n",
            "Epoch 801/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4244 - accuracy: 0.8038 - val_loss: 0.4957 - val_accuracy: 0.7917\n",
            "Epoch 802/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4244 - accuracy: 0.8038 - val_loss: 0.4957 - val_accuracy: 0.7917\n",
            "Epoch 803/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4244 - accuracy: 0.8056 - val_loss: 0.4958 - val_accuracy: 0.7917\n",
            "Epoch 804/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4243 - accuracy: 0.8038 - val_loss: 0.4958 - val_accuracy: 0.7917\n",
            "Epoch 805/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4243 - accuracy: 0.8038 - val_loss: 0.4958 - val_accuracy: 0.7917\n",
            "Epoch 806/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4242 - accuracy: 0.8038 - val_loss: 0.4958 - val_accuracy: 0.7917\n",
            "Epoch 807/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4242 - accuracy: 0.8038 - val_loss: 0.4958 - val_accuracy: 0.7917\n",
            "Epoch 808/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4242 - accuracy: 0.8038 - val_loss: 0.4958 - val_accuracy: 0.7917\n",
            "Epoch 809/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4242 - accuracy: 0.8038 - val_loss: 0.4958 - val_accuracy: 0.7917\n",
            "Epoch 810/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4241 - accuracy: 0.8038 - val_loss: 0.4958 - val_accuracy: 0.7917\n",
            "Epoch 811/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4241 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 812/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4240 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 813/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4240 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 814/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4240 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 815/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4239 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 816/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4239 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 817/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4239 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 818/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4239 - accuracy: 0.8056 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 819/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4238 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 820/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4238 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 821/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4238 - accuracy: 0.8056 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 822/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4237 - accuracy: 0.8038 - val_loss: 0.4959 - val_accuracy: 0.7917\n",
            "Epoch 823/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4237 - accuracy: 0.8038 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 824/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4237 - accuracy: 0.8038 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 825/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4236 - accuracy: 0.8038 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 826/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.8038 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 827/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.8056 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 828/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4235 - accuracy: 0.8038 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 829/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4235 - accuracy: 0.8038 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 830/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4234 - accuracy: 0.8038 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 831/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4234 - accuracy: 0.8038 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 832/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4234 - accuracy: 0.8056 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 833/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4234 - accuracy: 0.8073 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 834/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4233 - accuracy: 0.8056 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 835/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4233 - accuracy: 0.8073 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 836/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4233 - accuracy: 0.8073 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
            "Epoch 837/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4232 - accuracy: 0.8056 - val_loss: 0.4961 - val_accuracy: 0.7917\n",
            "Epoch 838/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4232 - accuracy: 0.8056 - val_loss: 0.4961 - val_accuracy: 0.7917\n",
            "Epoch 839/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4232 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7917\n",
            "Epoch 840/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4231 - accuracy: 0.8090 - val_loss: 0.4961 - val_accuracy: 0.7917\n",
            "Epoch 841/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4231 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7917\n",
            "Epoch 842/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4231 - accuracy: 0.8090 - val_loss: 0.4961 - val_accuracy: 0.7917\n",
            "Epoch 843/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4230 - accuracy: 0.8090 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 844/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4230 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 845/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4230 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 846/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4230 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 847/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.8090 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 848/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4229 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 849/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4229 - accuracy: 0.8090 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 850/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4229 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 851/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4228 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 852/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4228 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 853/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4227 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 854/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4227 - accuracy: 0.8073 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 855/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4227 - accuracy: 0.8090 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 856/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4227 - accuracy: 0.8090 - val_loss: 0.4961 - val_accuracy: 0.7865\n",
            "Epoch 857/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4226 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 858/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4226 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 859/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4226 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 860/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4226 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 861/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4225 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 862/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4225 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 863/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4225 - accuracy: 0.8056 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 864/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4224 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 865/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4224 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 866/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4224 - accuracy: 0.8090 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 867/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4223 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 868/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4223 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 869/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4223 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 870/1000\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4222 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 871/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4222 - accuracy: 0.8090 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 872/1000\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4222 - accuracy: 0.8090 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 873/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4222 - accuracy: 0.8090 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 874/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4222 - accuracy: 0.8090 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 875/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4221 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 876/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4221 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
            "Epoch 877/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4221 - accuracy: 0.8073 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 878/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4220 - accuracy: 0.8073 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 879/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4220 - accuracy: 0.8090 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 880/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4219 - accuracy: 0.8090 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 881/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4220 - accuracy: 0.8073 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 882/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4219 - accuracy: 0.8090 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 883/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4219 - accuracy: 0.8073 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 884/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4218 - accuracy: 0.8073 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 885/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4218 - accuracy: 0.8090 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 886/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4218 - accuracy: 0.8073 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 887/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4218 - accuracy: 0.8073 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 888/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4217 - accuracy: 0.8090 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 889/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4217 - accuracy: 0.8073 - val_loss: 0.4963 - val_accuracy: 0.7865\n",
            "Epoch 890/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4216 - accuracy: 0.8090 - val_loss: 0.4964 - val_accuracy: 0.7865\n",
            "Epoch 891/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4216 - accuracy: 0.8073 - val_loss: 0.4964 - val_accuracy: 0.7865\n",
            "Epoch 892/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4216 - accuracy: 0.8090 - val_loss: 0.4964 - val_accuracy: 0.7865\n",
            "Epoch 893/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4215 - accuracy: 0.8090 - val_loss: 0.4964 - val_accuracy: 0.7865\n",
            "Epoch 894/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4215 - accuracy: 0.8090 - val_loss: 0.4964 - val_accuracy: 0.7865\n",
            "Epoch 895/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4215 - accuracy: 0.8090 - val_loss: 0.4964 - val_accuracy: 0.7865\n",
            "Epoch 896/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4215 - accuracy: 0.8073 - val_loss: 0.4964 - val_accuracy: 0.7865\n",
            "Epoch 897/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4215 - accuracy: 0.8090 - val_loss: 0.4964 - val_accuracy: 0.7865\n",
            "Epoch 898/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4214 - accuracy: 0.8073 - val_loss: 0.4964 - val_accuracy: 0.7865\n",
            "Epoch 899/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4214 - accuracy: 0.8090 - val_loss: 0.4965 - val_accuracy: 0.7865\n",
            "Epoch 900/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4213 - accuracy: 0.8073 - val_loss: 0.4965 - val_accuracy: 0.7865\n",
            "Epoch 901/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4213 - accuracy: 0.8090 - val_loss: 0.4965 - val_accuracy: 0.7865\n",
            "Epoch 902/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4213 - accuracy: 0.8090 - val_loss: 0.4965 - val_accuracy: 0.7865\n",
            "Epoch 903/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4212 - accuracy: 0.8090 - val_loss: 0.4965 - val_accuracy: 0.7865\n",
            "Epoch 904/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4212 - accuracy: 0.8090 - val_loss: 0.4965 - val_accuracy: 0.7865\n",
            "Epoch 905/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4212 - accuracy: 0.8090 - val_loss: 0.4965 - val_accuracy: 0.7865\n",
            "Epoch 906/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4211 - accuracy: 0.8090 - val_loss: 0.4965 - val_accuracy: 0.7865\n",
            "Epoch 907/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4211 - accuracy: 0.8090 - val_loss: 0.4966 - val_accuracy: 0.7865\n",
            "Epoch 908/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4211 - accuracy: 0.8090 - val_loss: 0.4966 - val_accuracy: 0.7865\n",
            "Epoch 909/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4210 - accuracy: 0.8090 - val_loss: 0.4966 - val_accuracy: 0.7865\n",
            "Epoch 910/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4210 - accuracy: 0.8090 - val_loss: 0.4966 - val_accuracy: 0.7865\n",
            "Epoch 911/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4210 - accuracy: 0.8090 - val_loss: 0.4966 - val_accuracy: 0.7865\n",
            "Epoch 912/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4209 - accuracy: 0.8090 - val_loss: 0.4966 - val_accuracy: 0.7865\n",
            "Epoch 913/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4209 - accuracy: 0.8090 - val_loss: 0.4966 - val_accuracy: 0.7865\n",
            "Epoch 914/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4209 - accuracy: 0.8090 - val_loss: 0.4966 - val_accuracy: 0.7865\n",
            "Epoch 915/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4209 - accuracy: 0.8090 - val_loss: 0.4966 - val_accuracy: 0.7865\n",
            "Epoch 916/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4208 - accuracy: 0.8090 - val_loss: 0.4967 - val_accuracy: 0.7865\n",
            "Epoch 917/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4208 - accuracy: 0.8090 - val_loss: 0.4967 - val_accuracy: 0.7865\n",
            "Epoch 918/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4207 - accuracy: 0.8090 - val_loss: 0.4967 - val_accuracy: 0.7865\n",
            "Epoch 919/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8073 - val_loss: 0.4967 - val_accuracy: 0.7865\n",
            "Epoch 920/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8090 - val_loss: 0.4967 - val_accuracy: 0.7865\n",
            "Epoch 921/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8073 - val_loss: 0.4967 - val_accuracy: 0.7865\n",
            "Epoch 922/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4206 - accuracy: 0.8090 - val_loss: 0.4967 - val_accuracy: 0.7865\n",
            "Epoch 923/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4206 - accuracy: 0.8073 - val_loss: 0.4968 - val_accuracy: 0.7865\n",
            "Epoch 924/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4205 - accuracy: 0.8090 - val_loss: 0.4968 - val_accuracy: 0.7865\n",
            "Epoch 925/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4205 - accuracy: 0.8073 - val_loss: 0.4968 - val_accuracy: 0.7865\n",
            "Epoch 926/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4205 - accuracy: 0.8073 - val_loss: 0.4968 - val_accuracy: 0.7865\n",
            "Epoch 927/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4204 - accuracy: 0.8073 - val_loss: 0.4968 - val_accuracy: 0.7865\n",
            "Epoch 928/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4204 - accuracy: 0.8073 - val_loss: 0.4968 - val_accuracy: 0.7865\n",
            "Epoch 929/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4204 - accuracy: 0.8090 - val_loss: 0.4968 - val_accuracy: 0.7865\n",
            "Epoch 930/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4203 - accuracy: 0.8090 - val_loss: 0.4969 - val_accuracy: 0.7865\n",
            "Epoch 931/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4203 - accuracy: 0.8108 - val_loss: 0.4969 - val_accuracy: 0.7865\n",
            "Epoch 932/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4202 - accuracy: 0.8090 - val_loss: 0.4969 - val_accuracy: 0.7865\n",
            "Epoch 933/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4202 - accuracy: 0.8090 - val_loss: 0.4969 - val_accuracy: 0.7865\n",
            "Epoch 934/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4202 - accuracy: 0.8108 - val_loss: 0.4969 - val_accuracy: 0.7865\n",
            "Epoch 935/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4201 - accuracy: 0.8073 - val_loss: 0.4970 - val_accuracy: 0.7865\n",
            "Epoch 936/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4201 - accuracy: 0.8090 - val_loss: 0.4970 - val_accuracy: 0.7865\n",
            "Epoch 937/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4201 - accuracy: 0.8090 - val_loss: 0.4970 - val_accuracy: 0.7865\n",
            "Epoch 938/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4201 - accuracy: 0.8090 - val_loss: 0.4970 - val_accuracy: 0.7865\n",
            "Epoch 939/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4200 - accuracy: 0.8073 - val_loss: 0.4970 - val_accuracy: 0.7865\n",
            "Epoch 940/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4200 - accuracy: 0.8090 - val_loss: 0.4971 - val_accuracy: 0.7865\n",
            "Epoch 941/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4200 - accuracy: 0.8090 - val_loss: 0.4971 - val_accuracy: 0.7865\n",
            "Epoch 942/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4200 - accuracy: 0.8090 - val_loss: 0.4971 - val_accuracy: 0.7865\n",
            "Epoch 943/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4199 - accuracy: 0.8073 - val_loss: 0.4971 - val_accuracy: 0.7865\n",
            "Epoch 944/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4199 - accuracy: 0.8090 - val_loss: 0.4971 - val_accuracy: 0.7865\n",
            "Epoch 945/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4198 - accuracy: 0.8073 - val_loss: 0.4972 - val_accuracy: 0.7865\n",
            "Epoch 946/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4198 - accuracy: 0.8090 - val_loss: 0.4972 - val_accuracy: 0.7865\n",
            "Epoch 947/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4197 - accuracy: 0.8056 - val_loss: 0.4972 - val_accuracy: 0.7865\n",
            "Epoch 948/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4197 - accuracy: 0.8090 - val_loss: 0.4972 - val_accuracy: 0.7865\n",
            "Epoch 949/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.8073 - val_loss: 0.4972 - val_accuracy: 0.7865\n",
            "Epoch 950/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4196 - accuracy: 0.8073 - val_loss: 0.4973 - val_accuracy: 0.7865\n",
            "Epoch 951/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4196 - accuracy: 0.8073 - val_loss: 0.4973 - val_accuracy: 0.7865\n",
            "Epoch 952/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4196 - accuracy: 0.8073 - val_loss: 0.4973 - val_accuracy: 0.7865\n",
            "Epoch 953/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4196 - accuracy: 0.8090 - val_loss: 0.4973 - val_accuracy: 0.7865\n",
            "Epoch 954/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4195 - accuracy: 0.8073 - val_loss: 0.4973 - val_accuracy: 0.7865\n",
            "Epoch 955/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4195 - accuracy: 0.8056 - val_loss: 0.4974 - val_accuracy: 0.7865\n",
            "Epoch 956/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4195 - accuracy: 0.8073 - val_loss: 0.4974 - val_accuracy: 0.7865\n",
            "Epoch 957/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4194 - accuracy: 0.8073 - val_loss: 0.4974 - val_accuracy: 0.7865\n",
            "Epoch 958/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4194 - accuracy: 0.8073 - val_loss: 0.4974 - val_accuracy: 0.7865\n",
            "Epoch 959/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4193 - accuracy: 0.8073 - val_loss: 0.4974 - val_accuracy: 0.7865\n",
            "Epoch 960/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4193 - accuracy: 0.8073 - val_loss: 0.4974 - val_accuracy: 0.7865\n",
            "Epoch 961/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4193 - accuracy: 0.8073 - val_loss: 0.4975 - val_accuracy: 0.7865\n",
            "Epoch 962/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4193 - accuracy: 0.8073 - val_loss: 0.4975 - val_accuracy: 0.7865\n",
            "Epoch 963/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4192 - accuracy: 0.8073 - val_loss: 0.4975 - val_accuracy: 0.7865\n",
            "Epoch 964/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4192 - accuracy: 0.8090 - val_loss: 0.4975 - val_accuracy: 0.7865\n",
            "Epoch 965/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4192 - accuracy: 0.8073 - val_loss: 0.4976 - val_accuracy: 0.7865\n",
            "Epoch 966/1000\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4191 - accuracy: 0.8073 - val_loss: 0.4976 - val_accuracy: 0.7865\n",
            "Epoch 967/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4191 - accuracy: 0.8090 - val_loss: 0.4976 - val_accuracy: 0.7865\n",
            "Epoch 968/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4190 - accuracy: 0.8073 - val_loss: 0.4976 - val_accuracy: 0.7865\n",
            "Epoch 969/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4190 - accuracy: 0.8090 - val_loss: 0.4976 - val_accuracy: 0.7865\n",
            "Epoch 970/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4190 - accuracy: 0.8090 - val_loss: 0.4977 - val_accuracy: 0.7865\n",
            "Epoch 971/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4189 - accuracy: 0.8108 - val_loss: 0.4977 - val_accuracy: 0.7865\n",
            "Epoch 972/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4189 - accuracy: 0.8090 - val_loss: 0.4977 - val_accuracy: 0.7865\n",
            "Epoch 973/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4188 - accuracy: 0.8108 - val_loss: 0.4977 - val_accuracy: 0.7865\n",
            "Epoch 974/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4188 - accuracy: 0.8108 - val_loss: 0.4978 - val_accuracy: 0.7865\n",
            "Epoch 975/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4188 - accuracy: 0.8108 - val_loss: 0.4978 - val_accuracy: 0.7865\n",
            "Epoch 976/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4187 - accuracy: 0.8108 - val_loss: 0.4978 - val_accuracy: 0.7865\n",
            "Epoch 977/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4187 - accuracy: 0.8108 - val_loss: 0.4978 - val_accuracy: 0.7865\n",
            "Epoch 978/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4187 - accuracy: 0.8108 - val_loss: 0.4979 - val_accuracy: 0.7865\n",
            "Epoch 979/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4187 - accuracy: 0.8090 - val_loss: 0.4979 - val_accuracy: 0.7865\n",
            "Epoch 980/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4186 - accuracy: 0.8108 - val_loss: 0.4979 - val_accuracy: 0.7865\n",
            "Epoch 981/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4186 - accuracy: 0.8108 - val_loss: 0.4979 - val_accuracy: 0.7865\n",
            "Epoch 982/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4185 - accuracy: 0.8108 - val_loss: 0.4980 - val_accuracy: 0.7865\n",
            "Epoch 983/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4185 - accuracy: 0.8108 - val_loss: 0.4980 - val_accuracy: 0.7865\n",
            "Epoch 984/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4185 - accuracy: 0.8108 - val_loss: 0.4980 - val_accuracy: 0.7865\n",
            "Epoch 985/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4184 - accuracy: 0.8108 - val_loss: 0.4980 - val_accuracy: 0.7865\n",
            "Epoch 986/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4184 - accuracy: 0.8108 - val_loss: 0.4981 - val_accuracy: 0.7865\n",
            "Epoch 987/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4183 - accuracy: 0.8108 - val_loss: 0.4981 - val_accuracy: 0.7865\n",
            "Epoch 988/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4183 - accuracy: 0.8108 - val_loss: 0.4981 - val_accuracy: 0.7865\n",
            "Epoch 989/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4183 - accuracy: 0.8108 - val_loss: 0.4981 - val_accuracy: 0.7865\n",
            "Epoch 990/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4183 - accuracy: 0.8108 - val_loss: 0.4982 - val_accuracy: 0.7865\n",
            "Epoch 991/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4182 - accuracy: 0.8108 - val_loss: 0.4982 - val_accuracy: 0.7865\n",
            "Epoch 992/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4182 - accuracy: 0.8108 - val_loss: 0.4982 - val_accuracy: 0.7865\n",
            "Epoch 993/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4181 - accuracy: 0.8108 - val_loss: 0.4983 - val_accuracy: 0.7865\n",
            "Epoch 994/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4181 - accuracy: 0.8108 - val_loss: 0.4983 - val_accuracy: 0.7865\n",
            "Epoch 995/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4180 - accuracy: 0.8108 - val_loss: 0.4983 - val_accuracy: 0.7865\n",
            "Epoch 996/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4180 - accuracy: 0.8108 - val_loss: 0.4983 - val_accuracy: 0.7865\n",
            "Epoch 997/1000\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4180 - accuracy: 0.8108 - val_loss: 0.4984 - val_accuracy: 0.7865\n",
            "Epoch 998/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4179 - accuracy: 0.8108 - val_loss: 0.4984 - val_accuracy: 0.7865\n",
            "Epoch 999/1000\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4179 - accuracy: 0.8108 - val_loss: 0.4984 - val_accuracy: 0.7865\n",
            "Epoch 1000/1000\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4179 - accuracy: 0.8108 - val_loss: 0.4985 - val_accuracy: 0.7865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "Wmz0BrLURcR7",
        "outputId": "1b53162f-7ea7-4a85-d479-39b42831b50f"
      },
      "source": [
        "n = len(run_hist_1.history[\"loss\"])\n",
        "m = len(run_hist_1b.history['loss'])\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
        "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
        "\n",
        "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
        "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
        "\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fcafd763b90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dpet2vURcR7"
      },
      "source": [
        "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "SDIDki8IRcR7",
        "outputId": "2898a7b9-25d4-46c5-f680-786e4fad74f2"
      },
      "source": [
        "y_pred_class_nn_2 = model_1.predict_classes(X_test_norm)\n",
        "y_pred_prob_nn_2 = model_1.predict(X_test_norm)\n",
        "# Print model performance and plot the roc curve\n",
        "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_2)))\n",
        "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_2)))\n",
        "\n",
        "plot_roc(y_test, y_pred_prob_nn_2, 'NN')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 0.786\n",
            "roc-auc is 0.826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rj4a9QbqRcR7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InoTmEWaRcR8"
      },
      "source": [
        "## Exercise\n",
        "Now it's your turn.  Do the following in the cells below:\n",
        "- Build a model, model_2, with two hidden layers, each with 6 nodes\n",
        "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
        "- Use a learning rate of .003 and train for 1000 epochs\n",
        "- Graph the trajectory of the loss functions and accuracy on both train and test set\n",
        "- Plot the roc curve for the predictions\n",
        "\n",
        "You might want to look at the Keras documentation at: https://keras.io/guides/sequential_model/\n",
        "\n",
        "I like the example in the section, \"Specifying the input shape in advance\".\n",
        "\n",
        "Experiment with one network with 3 layers and save it as model_3. Did it work better?\n",
        "Did using more or less epochs help?\n",
        "Trying different learning rates for model_3.  Did that work better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iNRdeh9CRcR8"
      },
      "source": [
        "# Type your code here for model_2 with layers 1,2 having activation relu and layer 3 with activation sigmoid\n",
        "# Define the Model \n",
        "# Input size is 8-dimensional\n",
        "model_2 = Sequential([\n",
        "    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7ll7BXWBKO6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_ZqaFL5zKPVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1SS78JASgkc"
      },
      "source": [
        "# Graph the trajectory of the loss functions and accuracy on both train and test set for model_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gY9lUoxaRcR8"
      },
      "source": [
        "# Type your code here to plot the loss, accuracy and ROC curve for model_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOWpJM2ZSK2W"
      },
      "source": [
        "# Type your code here for model_3 with layers 1,2,3 having activation relu and layer 4 with activation sigmoid\n",
        "# Define the Model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEcXpvh5R-s6"
      },
      "source": [
        "# Type your code here to plot the loss, accuracy and ROC curve for model_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le_OBVUGSgkc"
      },
      "source": [
        "# Try using more or less epochs and different learning rates for model_3 "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}